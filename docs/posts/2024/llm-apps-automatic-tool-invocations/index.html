<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta name="description" content="In the previous post we demonstrated how instructions embedded in untrusted data can invoke ChatGPT&rsquo;s memory tool. The examples we looked at included â€¦" />
  <link rel="canonical" href="https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/" />
  <meta property="og:title" content=" Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations &middot;  Embrace The Red" />
  <meta property="og:description" content="In the previous post we demonstrated how instructions embedded in untrusted data can invoke ChatGPT&rsquo;s memory tool. The examples we looked at included â€¦" />
  <meta property="og:site_name" content="Embrace The Red" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/" />
  <meta property="og:type" content="article" />
  <meta property="og:article:published_time" content="2024-05-28T20:57:38-07:00" />
  <meta property="og:article:tag" content="ai" />
  <meta property="og:article:tag" content="testing" />
  <meta property="og:article:tag" content="machine learning" />
  <meta property="og:article:tag" content="prompt injection" />
  <meta property="og:article:tag" content="chatgpt" />
  <meta property="og:article:tag" content="ttp" />

  <title>
     Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations &middot;  Embrace The Red
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@wunderwuzzi23">
<meta name="twitter:title" content="Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations">
<meta name="twitter:description" content="Automatic Tool Invocation when Browsing with ChatGPT remains a risk to be aware of. Unfortunately it is possible to invoke the tools like memory or DALLE directly during prompt injection.">
<meta name="twitter:image" content="https://embracethered.com/blog/images/2024/chatgpt-ati-thumbnail.png">

<meta name="twitter:creator" content="@wunderwuzzi23">



  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "\"Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations\"",
    "description": "\"In the previous post we demonstrated how instructions embedded in untrusted data can invoke ChatGPT\\u0026rsquo;s memory tool. The examples we looked at included â€¦\"",
    "author": {
      "@type": "Person",
      "name": "wunderwuzzi",
      "url": "https://x.com/wunderwuzzi23"
    },
    "publisher": {
      "@type": "Organization",
      "name": "\"Embrace The Red\"",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/embracethered.com\/blog\/images/favicon.ico"
      }
    },
    "url": "\"https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/\"",
    "datePublished": "\"2024-05-28T20:57:38-07:00\"","dateModified": "\"2024-05-28T20:57:38-07:00\"",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "\"https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/\""
    }
  }
  </script>

</head>
<body>
    <header class="global-header"  style="background-image:url( /blog/images/bg.png )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">Embrace The Red</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        wunderwuzzi&#39;s blog
        <br><div style="color: greenyellow; font-weight:400;">learn the hacks, stop the attacks.</div> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-rss hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;RSS
      </a>
      <a data-formkit-toggle="15376e9e24" href="https://embracethered.kit.com/15376e9e24" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-envelope" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2024-05-28T20:57:38-07:00">
          May 28, 2024
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/ai">#ai</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/testing">#testing</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/prompt-injection">#prompt injection</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/chatgpt">#chatgpt</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/ttp">#ttp</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>In the <a href="/blog/posts/2024/chatgpt-hacking-memories/">previous post</a> we demonstrated how instructions embedded in untrusted data can invoke ChatGPT&rsquo;s memory tool. The examples we looked at included <code>Uploaded Files</code>, <code>Connected Apps</code> and also the <code>Browsing</code> tool.</p>
<p><a href="/blog/images/2024//chatgpt-ati2.png"><img src="/blog/images/2024/chatgpt-ati2.png" alt="image tool invocation"></a></p>
<p>When it came to the browsing tool we observed that mitigations were put in place and older demo exploits did not work anymore. After chatting with other security researchers, I learned that they had observed the same.</p>
<p><strong>However, with some minor prompting tricks mitigations are bypassed and we, again, can demonstrate automatic tool invocation when browsing the Internet with ChatGPT!</strong></p>
<p>This follow up post, as promised, shares details on the status of prompt injection and automated tool invocation when ChatGPT surfs the Internet.</p>
<h2 id="automatic-tool-invocation-when-browsing">Automatic Tool Invocation when Browsing</h2>
<p>Although this post focuses on exploits with OpenAI&rsquo;s ChatGPT, such security vulnerabilities are present in other LLM applications and agents.</p>
<p>The exploit chain is also reflected in the <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf">OWASP Top 10 for LLMs</a> in particular concerning these threats:</p>
<ul>
<li><code>LLM01: Prompt Injection</code></li>
<li><code>LLM07: Insecure Plugin Design</code></li>
<li><code>LLM08: Excessive Agency</code></li>
</ul>
<h3 id="try-for-yourself">Try For Yourself!</h3>
<p>If you want you can also try it yourself, all that is needed is ChatGPT 4o.</p>
<p><strong>1. Invoking Image Creation with DALLE</strong></p>
<p>A fun demo is having instructions on a website invoke DALLE! Just paste this URL into a new chat and see what happens: <code>https://wuzzi.net/create.txt</code>.</p>
<p><a href="/blog/images/2024//chatgpt-invoke-dalle2.png"><img src="/blog/images/2024/chatgpt-invoke-dalle2.png" alt="image creation"></a></p>
<p>Look! ChatGPT just created an image!</p>
<p>If you want to see something even more interesting, the next example will invoke DALLE and create an image based on <strong>your stored memories</strong>: <code>https://wuzzi.net/8create.txt</code></p>
<p><em>Use with caution as personal information might be placed into the image by ChatGPT, alternatively just watch the video further below for a demo.</em></p>
<p>Cool, right? This works because all the data from the website, the user&rsquo;s written text, and the initial prompt (system prompt), including the memories, are in the same trust boundary.</p>
<p>Speaking of the memory tool, let&rsquo;s look at that next.</p>
<p><strong>2. Invoke the Memory Tool</strong></p>
<p>We can also invoke the memory tool by placing specific instructions on a website:</p>
<p><a href="/blog/images/2024/chatgpt-invoke-tools-annotated.png"><img src="/blog/images/2024/chatgpt-invoke-tools-annotated.png" alt="add and remove memories"></a></p>
<p><strong>Add memories:</strong> <code>https://wuzzi.net/c/add.txt</code></p>
<p><em>Note: I noticed that above might not work if there are already memories present, you can also try <code>https://wuzzi.net/c/do2-2.txt</code> which is more reliable when there are already memories stored.</em></p>
<p><strong>Delete memories:</strong> This URL will delete your memories: <code>https://wuzzi.net/c/rm.txt</code></p>
<p>At the moment these are quite reliable and work close to 100% at the moment.</p>
<h3 id="prompt-injection-instructions">Prompt Injection Instructions</h3>
<p>Let&rsquo;s review the contents of the websites, which trick ChatGPT into calling tools. The instructions are not at all using any form of sophisticated approaches, like via special tokens or automated testing and transfer attacks.</p>
<p>The first one is the creation of an image based on memories:</p>
<p><a href="/blog/images/2024/chatgpt-invoke-prompt-injection.png"><img src="/blog/images/2024/chatgpt-invoke-prompt-injection.png" alt="prompt injection payload"></a></p>
<p>This one, which I really like, is for clearing of memories:</p>
<p><a href="/blog/images/2024/chatgpt-prompt-bypass.png"><img src="/blog/images/2024/chatgpt-prompt-bypass.png" alt="add and remove memories"></a></p>
<p>Both are great examples on how levels of indirection easily &ldquo;trick&rdquo; language models into following instructions. LLMs are naive and gullible. The second demo adds a twist with stating a challenge.</p>
<h3 id="video-demonstration">Video Demonstration</h3>
<p>This video shows the three POCs in a quick run through:</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/j72rT-QAjuk?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>The three examples are:</p>
<ol>
<li>Website instructions <strong>add new long term memories</strong> ðŸ§ </li>
<li>Instructions on website <strong>invoke DALLE</strong> to create an image based on stored memories</li>
<li>Instructions from website <strong>erase all your precious memories</strong> ðŸ¤¯</li>
</ol>
<p>If you followed along with the ChatGPT security journey, you might wonder about plugins&hellip;</p>
<h3 id="so-what-about-plugins">So, what about plugins?</h3>
<p><strong>Plugins are gone now!</strong></p>
<p>The replacement for plugins are GPTs with <code>AI Actions</code>.</p>
<h3 id="so-what-about-custom-gpts-and-ai-actions">So, what about custom GPTs and AI Actions?</h3>
<p>Custom GPTs can have <code>AI Actions</code> and those can be vulnerable as well!</p>
<p>The good news, there is a <a href="https://platform.openai.com/docs/actions/getting-started">mitigation GPT creators can use</a> with the <code>x-openai-isConsequential</code> flag!</p>
<p><a href="/blog/images/2024/chatgpt-matrix-is-consequential-true.png"><img src="/blog/images/2024/chatgpt-matrix-is-consequential-true.png" alt="is-consequential"></a></p>
<p>As you can see from the screenshot, it still lacks a great user experience, like better visualization to understand what the action is about to do, but it is a mitigation and puts the user in control.</p>
<h2 id="mitigations-and-recommendations">Mitigations and Recommendations</h2>
<p>It is surprising that OpenAI has not entirely tackled the problem of tool invocation with their built-in tools, especially when the memory feature was added. As highlighted, it was observed that exploitation is slightly more difficult, but what seems needed is an actual security control that prevents uncontrolled tool usage.</p>
<p>A year ago I brought this up, and most suggestions remain the same and OpenAI implemented some basic mitigations as suggested (like the introduction of the <code>x-openai-isConsequential</code> flag):</p>
<ul>
<li>Do not automatically invoke sensitive or powerful tools once untrusted data is in the context.</li>
<li>Establishing basic security controls such as integrity levels, tagging of data and taint tracking to help track untrusted data and highlight the integrity level/trustworthiness of a chat.</li>
<li>There is a remaining risk that even with integrity levels and taint tracking hallucinations might randomly invoke a tool!</li>
<li>For the browsing tool specifically, it might also help to not directly connect to websites but leverage Bing&rsquo;s index/condensed information -&gt; fewer degrees of freedom.</li>
<li>Creation and documentation of trust boundaries help understand the security contract, and if a discovered behavior is an accepted risk by a vendor (that needs to be communicated to users) or a an unintended flaw in the system.</li>
<li>&ldquo;Content Security Policy&rdquo; frameworks could allow better configuration and definitions for AI systems, for instances which sources/domains are trusted and which ones are not.</li>
<li>The usage of additional LLMs (and other content moderation systems) to validate if the returned text still fulfills the initial objectives. This is actually something OpenAI might be doing now, since some mitigations were observed recently.</li>
<li>A bit of a segue, Microsoft has <a href="https://techcommunity.microsoft.com/t5/azure-architecture-blog/security-best-practices-for-genai-applications-openai-in-azure/ba-p/4027885">Azure Purview</a> which could allow them to have solid mitigations down the road by tracking what kind of data enters a chat!</li>
</ul>
<p>Finally, one thing to keep in mind&hellip; intuition tells me, that as capabilities of language models increase, so will the bypass capabilities.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Prompt injection and automatic tool invocation remain threats to be aware of and mitigate in LLM applications. In this post we <a href="/blog/posts/2024/chatgpt-hacking-memories/">continued from the previous post</a> and explored how newly added features (like memory) can be exploited using these techniques via the browsing tool, and we observed that actual security controls are still lacking behind.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="/blog/posts/2024/chatgpt-hacking-memories/">Hacking Memories with Prompt Injection</a></li>
<li><a href="https://platform.openai.com/docs/actions/getting-started">OpenAI Documentation - Is Consquential Flag</a></li>
<li><a href="https://simonwillison.net/2023/Apr/25/dual-llm-pattern/#confused-deputy-attacks">Simon Willison - Confused Deputy Attacks</a></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-architecture-blog/security-best-practices-for-genai-applications-openai-in-azure/ba-p/4027885">Azure Purview</a></li>
<li>Images and Thumbnails created with DALLE and manually edited</li>
</ul>

  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next"><a href="https://embracethered.com/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2018-2026
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. | <a href="https://embracethered.com/blog/privacy.txt">Privacy</a>
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />


<div class="kit-subscribe-wrapper">
  <script async data-uid="15376e9e24" src="https://embracethered.kit.com/15376e9e24/index.js"></script>
</div>

</body>
</html>

