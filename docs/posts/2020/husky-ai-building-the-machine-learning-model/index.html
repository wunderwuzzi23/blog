<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Husky AI: Building a machine learning system &middot;  Embrace The Red" />
  
  <meta property="og:site_name" content="Embrace The Red" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2020-09-04T12:04:29-07:00" />
  
  <meta property="og:article:tag" content="machine learning" />
  
  <meta property="og:article:tag" content="huskyai" />
  
  

  <title>
     Husky AI: Building a machine learning system &middot;  Embrace The Red
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  

  

  
</head>
<body>
    <header class="global-header"  style="background-image:url( /blog/images/bg.png )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">Embrace The Red</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        wunderwuzzi&#39;s blog
        <br><div style="color: greenyellow; font-weight:400;">learn the hacks, stop the attacks.</div> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Husky AI: Building a machine learning system</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2020-09-04T12:04:29-07:00">
          Sep 4, 2020
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/huskyai">#huskyai</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>This post is part of a series about machine learning and artificial intelligence.</p>
<p>In the <a href="/blog/posts/2020/husky-ai-walkthrough/">previous post</a> we described the overall machine learning pipeline.</p>
<p>In this post we dive into the technical details on how I built and trained the machine learning model for Husky AI.</p>
<p>After reading this you should have a good understanding around the technical steps involved in building a machine learning system, and also some thoughts around what can be attacked.</p>
<p><a href="/blog/images/2020/husky-ai.jpg"><img src="/blog/images/2020/husky-ai.jpg" alt="Husky AI"></a></p>
<h1 id="part2">Part 2 - Building the model</h1>
<p>Building a machine learning model is a very iterative process and takes a while.</p>
<p>Initially I started without TensorFlow and built a neural network, forward and backpropagation algorithms in Python. This is basically what the <code>Machine Learning</code> and <code>deeplearning.ai</code> courses I took teach you to do. I liked this as it gives a good understanding on what is going on under the hood.</p>
<p>The first step is gathering training data - images in the case of building Husky AI.</p>
<h2 id="gathering-training-and-validation-data">Gathering training and validation data</h2>
<p>To gather training data for husky and non-husky images I used Bing to gather images and did some manual data cleansing. It was about 1300 husky images and 3000 random other ones, including other dogs, people, and other objects.</p>
<p>I used Bing&rsquo;s image search to gather images. Check out <a href="https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/">Azure Cognitive Services and Bing Image Search</a>. They offer a free API to search and download images, all you must do is create an Azure account (which is also free) and then you can request an API key.</p>
<h3 id="source-code-to-azure-cognitive-services-bing-image-search">Source code to Azure Cognitive Services (Bing Image Search)</h3>
<p>I published code to call the Bing search API <a href="https://github.com/wunderwuzzi23/ai/blob/master/huskyai/scraper/bing-image-search.py">here</a> if you are interested to try it also.</p>
<h3 id="training-and-validation-data">Training and validation data</h3>
<p>Images are typically stored in dedicated folders for training and validation sets, and according to there label. This makes it easier later on because ML frameworks can use the &ldquo;folder name&rdquo; as the label for images. This is one of these typically pre-processing steps, where we get all the ducks, aehm, huskies in a row.</p>
<p>For instance, I have the following directory structure for images:</p>
<pre tabindex="0"><code>   data/training/husky
   data/training/nothusky
   data/validation/husky
   data/validation/nothusky
</code></pre><p>In machine learning data is split into separate sets. There are folders for <code>training data</code> and <code>validation data</code>. This is needed because we do not want to train our model on validation or test data. Validation and test data are used to tell us how well the model is performing on images <strong>not</strong> seen before.</p>
<p><strong>Attack idea:</strong> Attacker can attempt to poison either set of images, which can lead to vastly different results.</p>
<h3 id="exploring-the-images">Exploring the images</h3>
<p><img src="/blog/images/2020/huskyai-sampling.jpg" alt="Husky AI Sampling"></p>
<h2 id="building-the-model">Building the model</h2>
<p>The first model I built had only one layer and was basically just doing what is called &ldquo;logistic regression&rdquo; with a binary classifier.</p>
<p>It gave about a 62% accuracy, which I thought was good already. Although, I wanted to apply more of the knowledge I was learning from Andrew Ng&rsquo;s class and played around with a lot of different and more complex variations. Including regularization, convolutions, dropouts and things along those lines.</p>
<p>In the end I ended up with a convolutional neural network (CNN) with the following layout.</p>
<pre tabindex="0"><code>model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation=&#34;relu&#34;, input_shape=(num_px, num_px, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.4),
    
    tf.keras.layers.Conv2D(64, (3,3), activation=&#34;relu&#34;, input_shape=(num_px, num_px, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),

    tf.keras.layers.Dropout(0.4),
    
    tf.keras.layers.Conv2D(128, (3,3), activation=&#34;relu&#34;),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),
    
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=&#34;relu&#34;),
    tf.keras.layers.Dense(1, activation=&#34;sigmoid&#34;)
])

opt = tf.keras.optimizers.Adam(learning_rate=0.0005)
model.compile(optimizer=opt, loss=&#34;binary_crossentropy&#34;, metrics=[&#34;accuracy&#34;])
</code></pre><p>For the optimizer I played around with <code>RMSProp</code>, but decided on <code>Adam</code> eventually. Here is a summary of the deep neural network that I&rsquo;m using at the moment:</p>
<pre tabindex="0"><code>&gt;&gt; model.summary()

Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 126, 126, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         
_________________________________________________________________
dropout (Dropout)            (None, 63, 63, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 61, 61, 64)        18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 14, 14, 128)       0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               3211392   
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 129       
=================================================================
Total params: 3,304,769
Trainable params: 3,304,769
Non-trainable params: 0
</code></pre><p>If you are not familiar with neural networks these layer names might seem a bit mystical. To understand this in more detail, I recommend looking at <a href="blog/posts/2020/machine-learning-basics/">my previous post for some good learning material</a>.</p>
<h3 id="convolutions">Convolutions</h3>
<p>One thing to highlight is the use of convolutions in neural network (especially for image processing). Convolutions are a technique to apply filters to an image and as a result it makes neurons &ldquo;see&rdquo; specific features.</p>
<p>As an example, below you can see images while it goes through some of the hidden layers of the neural network. You can see in the how the convolutions seem to be focused on highlighting the ears of a husky. This is something specific the neural network now uses to figure out if the image contains a husky or not - pointy ears!</p>
<p><a href="/blog/images/2020/convolutions.jpg"><img src="/blog/images/2020/convolutions.jpg" alt="Husky Convolutions"></a></p>
<p>So much about the layout of the neural network itself.</p>
<h2 id="training-the-model">Training the model</h2>
<p>Training the model with Keras is pretty straight forward and is done via <code>model.fit</code> (or <code>model.fit_generator</code> respectively).</p>
<pre tabindex="0"><code>history = model.fit_generator(training_generator,
                              epochs=200, 
                              steps_per_epoch=80,
                              validation_data=validation_generator,
                              validation_steps=20,
                              callbacks=[callbacks],
                              verbose=1)
</code></pre><ul>
<li><strong>Input Training and Validation Data:</strong> I used <strong>generators</strong> for training and validation sets because I was loading images from the hard drive via an API called <code>flow_from_directory</code>. More about that a bit further down.</li>
<li><strong>Epochs:</strong> Other parts to specify are the training duration (number of epochs and steps).</li>
<li><strong>Callbacks</strong>: Passing a callback to the <code>fit</code> function allows to observe the training in more detail - in my case I used it to stop training when reaching a certain accuracy.</li>
</ul>
<p>The definition of the <strong>callback</strong> looks like this:</p>
<pre tabindex="0"><code>DESIRED_ACCURACY = 0.99

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get(&#34;accuracy&#34;) &gt; DESIRED_ACCURACY):
      print(f&#34;\nReached {DESIRED_ACCURACY}% accuracy - cancelling training.&#34;)
      self.model.stop_training = True

callbacks = myCallback()
</code></pre><p>After training for a few hours, the model had an accuracy on the validation set in the mid 70% range. Not bad, and I still wanted to apply <strong>Image Augmentation</strong> which I had just learned about in the <strong>deeplearning.ai</strong> course.</p>
<h3 id="image-augmentation">Image Augmentation</h3>
<p>Since my training set was rather smaller, I used a technique called <em>Image Augmentation</em>. Image Augmentation means that when training the model, the images feed to the model are updated on the fly. For instance, they are cropped, or flipped and things like that. This is to increase the training set and provide more variations. I thought that is a pretty neat trick.</p>
<p>I updated and played around with different algorithms, model structures as well as performing image augmentation. In TensorFlow this is done with the <code>ImageDataGenerator</code> and <code>flow_from_directory</code>.</p>
<pre tabindex="0"><code>training_datagen =  ImageDataGenerator(
    rescale=1/255,
    shear_range=0.2,
    zoom_range=0.2,
    rotation_range=30, 
    fill_mode=&#34;nearest&#34;,
    horizontal_flip=True)

training_generator = training_datagen.flow_from_directory(
    training_folder, 
    target_size=(num_px, num_px), 
    batch_size=batch_size,
    class_mode=&#39;binary&#39;)
</code></pre><p>By using Image Augmentation the model landed a ~84% accuracy on the validation set after a few hours of training.</p>
<p><img src="/blog/images/2020/huskyai.train.val.loss.png" alt="Training and Validation Loss"></p>
<h2 id="saving-the-model">Saving the model</h2>
<p>In Keras a model is saved with a call to <code>.save</code>. This includes the model structure and the weights.</p>
<pre tabindex="0"><code>model.save(&#34;huskymodel.h5&#34;)
</code></pre><p>There is also the API to only store the weights, called <code>.save_weights</code>. In that case you have to manually construct the model before loading the weights again. This is useful for saving checkpoints during training for instance.</p>
<p>In TensorFlow there are two file formats to choose from when saving models <code>.h5</code>, and also a <code>SaveModel</code>. For now we will just focus on the <code>.h5</code> file format.</p>
<p><strong>Attack idea:</strong> As you can image the model file itself is a pretty high value asset. It does contain both the layout of the neural network, as well as the model weights. Tampering with this file will allow to add backdoors and cause a lot of other issues. We will analyze this more during threat modeling.</p>
<h2 id="performing-predictions">Performing predictions</h2>
<p>To perform a prediction we load an image into memory and pass it to the model with <code>model.predict</code>.</p>
<p><a href="/blog/images/2020/husky-prediction.jpg"><img src="/blog/images/2020/husky-prediction.jpg" alt="Husky Prediction Example"></a></p>
<p>The model can still benefit from improvements (such as transfer learning, where we build on the shoulders of other more powerful pre-trained models). It has an accuracy of a bit over 80% on the validation data - which is okay for now.</p>
<h2 id="whats-next">What&rsquo;s next?</h2>
<p>Hopefully this was useful to get a better understanding around the moving bits and pieces involved in building a ML application. Next we will dive into the aspects of MLOps and we discusshow to <a href="/blog/posts/2020/husky-ai-mlops-operationalize-the-model">operationalize the model and put it behind a web application</a> and make it available to end users.</p>
<p>Twitter: <a href="https://twitter.com/wunderwuzzi23">@wunderwuzzi23</a></p>

  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next"><a href="https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2018-2025
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. 
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />
  
</body>
</html>

