<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Machine Learning Attack Series: Adversarial Robustness Toolbox Basics &middot;  Embrace The Red" />
  
  <meta property="og:site_name" content="Embrace The Red" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2020-10-22T15:00:48-07:00" />
  
  <meta property="og:article:tag" content="machine learning" />
  
  <meta property="og:article:tag" content="huskyai" />
  
  <meta property="og:article:tag" content="red" />
  
  

  <title>
     Machine Learning Attack Series: Adversarial Robustness Toolbox Basics &middot;  Embrace The Red
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" type="image/x-icon" href="https://embracethered.com/blog/images/favicon.ico" />
  

  

  
</head>
<body>
    <header class="global-header"  style="background-image:url( /blog/images/bg.png )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">Embrace The Red</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        wunderwuzzi&#39;s blog
        <br><a style="color: greenyellow; font-weight:300;text-decoration: underline; " href="https://www.amazon.com/gp/product/1838828869/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1838828869&linkCode=as2&tag=wunderwuzzi-20&linkId=b6523e937607be47499c6010ff489537">OUT NOW: Cybersecurity Attacks - Red Team Strategies</a> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Machine Learning Attack Series: Adversarial Robustness Toolbox Basics</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2020-10-22T15:00:48-07:00">
          Oct 22, 2020
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/huskyai">#huskyai</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/red">#red</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &ldquo;huskyai&rdquo; to see related posts.</p>
<ul>
<li><a href="/blog/posts/2020/machine-learning-attack-series-overview/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="/blog/posts/2020/husky-ai-threat-modeling-machine-learning/">Attacks</a>: Some of the attacks I want to investigate, learn about, and try out</li>
</ul>
<p>I wanted to explore the &ldquo;Adversarial Robustness Toolbox&rdquo; (ART) for a while to understand how it can be used to create adversarial examples for Husky AI.</p>
<p>A few weeks ago I described <a href="/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/">a range of perturbation attacks</a> to modify an existing image of the plush bunny to get it misclassified as a husky.</p>
<p><img src="/blog/images/2020/huskyai-shadowbunny.png" alt="Shadowbunny"></p>
<p>The goal of this post is to do the same, but leveraging ART.</p>
<h2 id="adversarial-robustness-toolbox-art">Adversarial Robustness Toolbox (ART)</h2>
<p><a href="https://adversarial-robustness-toolbox.org/">ART</a> was originially created by IBM and moved to the Linux AI Foundations in July 2020. It comes with a range of attack modules, as well as mitigation techniques and supports a wide range of machine learning frameworks.</p>
<p>The code of creating an adversarial examples is straight forward. There is <a href="https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#">good documentation</a> available also.</p>
<h2 id="basic-attack---fast-gradient-sign-method">Basic Attack - Fast Gradient Sign Method</h2>
<p>My initial goal was to just get a basic adversarial example using FGSM. To get started with Keras and ART first import a couple of modules.</p>
<pre tabindex="0"><code>from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import KerasClassifier
</code></pre><p>Interestingly, I had to disable eager execution in TensorFlow:</p>
<pre tabindex="0"><code>tf.compat.v1.disable_eager_execution()
</code></pre><p>Then load the model file:</p>
<pre tabindex="0"><code>model = keras.models.load_model(&#34;/content/huskymodel.h5&#34;)
</code></pre><p>And create an ART <code>KerasClassifier</code> for the attack:</p>
<pre tabindex="0"><code>classifier = KerasClassifier(model=model, clip_values=(0, 1), use_logits=False)
</code></pre><p>After that we have everything ready to perform an attack and create the adversarial example. Let&rsquo;s first load the image and print it&rsquo;s current prediction:</p>
<pre tabindex="0"><code>image = load_image(&#34;/tmp/shadowbunny.png&#34;)
predictions = classifier.predict(image)
print(predictions)
</code></pre><p>The result was <code>[[0.00195146]]</code>.</p>
<p>Now let&rsquo;s write ART code to perturbe the pixels using the FGSM attack:</p>
<pre tabindex="0"><code>image = load_image(&#34;/tmp/shadowbunny.png&#34;)

attacker = FastGradientMethod(estimator=classifier, eps=0.04, targeted=True)
x_attack = attacker.generate(x=image, y=[0.9])
predictions = classifier.predict(x=x_attack)

print(predictions)
plt.imshow(x_test_adv[0])
</code></pre><p>Here is a quick description of the steps in the above code:</p>
<ol>
<li>First we load the benign plush bunny image that we want misclassify</li>
<li>Then we create the FGSM attack instance. Note the API is called with <code>targeted=True</code>, which means we want the output to fall into a certain class/label. Husky AI uses a binary classifier and this was the way I got it to work.</li>
<li>Next we perform the attack and generate an image with perturbed pixels. Because <code>targeted=True</code> was specified we have to provide a label <code>y</code> value. Although in this case (binary classifier) I have not fully grasped what the <code>y</code> value is actually doing. The outcome is correct regardless of the actual value of <code>y</code>.</li>
<li>Finally we call <code>predict</code> on the newly created attack image <code>x_attack</code> and display it as well.</li>
</ol>
<p><a href="/blog/images/2020/art.shadowbunny.png"><img src="/blog/images/2020/art.shadowbunny.png" alt="ART FGSM Shadowbunny"></a></p>
<p>Notice how the prediction score is now at <code>66%</code>. The attack worked!
In this case its also visible that the image slightly changed &ndash; observe how the white background is not entirely plain white anymore.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Hope this quick introduction helps you get started with ART. I will be exploring a lot more going forward. Compared to the manual attacks and code I had to write in the previous posts about perturbations, the ART libraries makes these attacks much simpler to perform.</p>
<p>Twitter: <a href="https://twitter.com/wunderwuzzi23">@wunderwuzzi23</a></p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox">Adversarial Robustness Toolbox Source Code GitHub</a></li>
<li><a href="https://adversarial-robustness-toolbox.readthedocs.io">Adversarial Robustness Toolbox Documentation</a></li>
<li><a href="https://www.youtube.com/watch?v=TwP-gKBQyic">Adversarial Robustness - Theory and Practice (NeurIPS 2018 Tutorial)</a></li>
<li><a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">TensorFlow Adversarial FGSM</a></li>
</ul>

  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next"><a href="https://embracethered.com/blog/posts/2020/red-teaming-endpoint-protection-agent-edr/">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2020/hacking-the-matrix/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2018-2025
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  <a href="https://youtube.com/@embracethered" target="_blank">
    <i class="fa fa-youtube"></i>
  </a>
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. 
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />
  
</body>
</html>

