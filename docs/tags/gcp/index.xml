<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gcp on Embrace The Red</title>
    <link>http://localhost:1313/blog/tags/gcp/</link>
    <description>Recent content in Gcp on Embrace The Red</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2026</copyright>
    <lastBuildDate>Thu, 19 Oct 2023 06:35:37 -0700</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/tags/gcp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Google Cloud Vertex AI - Data Exfiltration Vulnerability Fixed in Generative AI Studio</title>
      <link>http://localhost:1313/blog/posts/2023/google-gcp-generative-ai-studio-data-exfiltration-fixed/</link>
      <pubDate>Thu, 19 Oct 2023 06:35:37 -0700</pubDate>
      <guid>http://localhost:1313/blog/posts/2023/google-gcp-generative-ai-studio-data-exfiltration-fixed/</guid>
      <description>&lt;p&gt;Large Language Model (LLM) applications and chatbots are quite commonly vulnerable to data exfiltration. In particular data exfiltration via &lt;code&gt;Image Markdown Injection&lt;/code&gt; is frequent.&lt;/p&gt;&#xA;&lt;p&gt;This post describes how Google Cloud&amp;rsquo;s Vertex AI - Generative AI Studio had this vulnerability that I responsibly disclosed and Google fixed.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;A big shout out to the Google Security team upfront, it took 22 minutes from report submission to receiving a confirmation from Google that this is a security issue that will be fixed.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
