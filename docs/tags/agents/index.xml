<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agents on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/agents/</link>
    <description>Recent content in Agents on Embrace The Red</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2025</copyright>
    <lastBuildDate>Sat, 30 Aug 2025 18:20:58 -0700</lastBuildDate>
    <atom:link href="https://embracethered.com/blog/tags/agents/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wrap Up: The Month of AI Bugs</title>
      <link>https://embracethered.com/blog/posts/2025/wrapping-up-month-of-ai-bugs/</link>
      <pubDate>Sat, 30 Aug 2025 18:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/wrapping-up-month-of-ai-bugs/</guid>
      <description>That&amp;rsquo;s it.&#xA;The Month of AI Bugs is done. There won&amp;rsquo;t be a post tomorrow, because I will be at PAX West.&#xA;Overview of Posts ChatGPT: Exfiltrating Your Chat History and Memories With Prompt Injection | Video ChatGPT Codex: Turning ChatGPT Codex Into a ZombAI Agent | Video Anthropic Filesystem MCP Server: Directory Access Bypass Via Improper Path Validation | Video Cursor: Arbitrary Data Exfiltration via Mermaid | Video Amp Code: Arbitrary Command Execution via Prompt Injection | Video Devin AI: I Spent $500 To Test Devin For Prompt Injection So That You Don&amp;rsquo;t Have To Devin AI: How Devin AI Can Leak Your Secrets via Multiple Means Devin AI: The AI Kill Chain in Action: Exposing Ports to the Internet via Prompt Injection OpenHands - The Lethal Trifecta Strikes Again: How Prompt Injection Can Leak Access Tokens OpenHands: Remote Code Execution and AI ClickFix Demo | Video Claude Code: Data Exfiltration with DNS Requests (CVE-2025-55284) | Video GitHub Copilot: Remote Code Execution (CVE-2025-53773) | Video Google Jules: Vulnerable to Multiple Data Exfiltration Issues Google Jules - Zombie Agent: From Prompt Injection to Remote Control Google Jules: Vulnerable To Invisible Prompt Injection Amp Code: Invisible Prompt Injection Vulnerability Fixed Amp Code: Data Exfiltration via Image Rendering Fixed | Video Amazon Q Developer: Secrets Leaked via DNS and Prompt Injection | Video Amazon Q Developer: Remote Code Execution via Prompt Injection | Video Amazon Q Developer: Vulnerable to Invisible Prompt Injection | Video Windsurf: Hijacking Windsurf: How Prompt Injection Leaks Developer Secrets | Video Windsurf: Memory-Persistent Data Exfiltration - SpAIware Exploit Windsurf: Sneaking Invisible Instructions by Developers Deep Research Agents: How Deep Research Agents Can Leak Your Data Manus: How Prompt Injection Hijacks Manus to Expose VS Code Server to the Internet | Video AWS Kiro: Arbitrary Code Execution via Indirect Prompt Injection | Video Cline: Vulnerable to Data Exfiltration and How to Protect Your Data | Video Windsurf MCP Integration: Missing Security Controls Put Users at Risk | Video Season Finale: AgentHopper: An AI Virus Research Project Demonstration | Video Thank you for following this research, and I hope it serves as a useful reference.</description>
    </item>
    <item>
      <title>AgentHopper: An AI Virus (Research Project)</title>
      <link>https://embracethered.com/blog/posts/2025/agenthopper-a-poc-ai-virus/</link>
      <pubDate>Fri, 29 Aug 2025 20:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/agenthopper-a-poc-ai-virus/</guid>
      <description>As part of the Month of AI Bugs, serious vulnerabilities that allow remote code execution via indirect prompt injection were discovered. There was a period of a few weeks where multiple arbitrary code execution vulnerabilities existed in popular agents, like GitHub Copilot, Amazon Q, AWS Kiro,&amp;hellip;&#xA;During that time I was wondering if it would be possible to write an AI virus.&#xA;Hence the idea of AgentHopper was born.</description>
    </item>
    <item>
      <title>Windsurf MCP Integration: Missing Security Controls Put Users at Risk</title>
      <link>https://embracethered.com/blog/posts/2025/windsurf-dangers-lack-of-security-controls-for-mcp-server-tool-invocation/</link>
      <pubDate>Thu, 28 Aug 2025 12:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/windsurf-dangers-lack-of-security-controls-for-mcp-server-tool-invocation/</guid>
      <description>Part of my default test cases for coding agents is to check how MCP integration looks like, especially if the agent can be configured to allow setting fine-grained controls for tools.&#xA;Sometimes there are basic security controls missing.&#xA;Especially when running an agent on your local computer. Stakes are much higher. And it seems important to empower users to be able to configure which actions an AI should be able to take automatically, and which ones should be suggestions that the user reviews before executing.</description>
    </item>
    <item>
      <title>Cline: Vulnerable To Data Exfiltration And How To Protect Your Data</title>
      <link>https://embracethered.com/blog/posts/2025/cline-vulnerable-to-data-exfiltration/</link>
      <pubDate>Wed, 27 Aug 2025 08:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/cline-vulnerable-to-data-exfiltration/</guid>
      <description>Cline is quite a popular AI coding agent, according to the product website it has 2+ million downloads and over 47k stars on GitHub.&#xA;Unfortunately, Cline is vulnerable to data exfiltration through the rendering of markdown images from untrusted domains in the chat box.&#xA;This allows an adversary to exfiltrate sensitive user information during a prompt injection attack by reading sensitive data (e.g. .env file) and appending its contents to the URL of an image.</description>
    </item>
    <item>
      <title>AWS Kiro: Arbitrary Code Execution via Indirect Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/aws-kiro-aribtrary-command-execution-with-indirect-prompt-injection/</link>
      <pubDate>Tue, 26 Aug 2025 07:00:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/aws-kiro-aribtrary-command-execution-with-indirect-prompt-injection/</guid>
      <description>On the day AWS Kiro was released, I couldn&amp;rsquo;t resist putting it through some of my Month of AI Bugs security tests for coding agents.&#xA;AWS Kiro was vulnerable to arbitrary command execution via indirect prompt injection. This means that a remote attacker, who controls data that Kiro processes, could hijack it to run arbitrary operating system commands or write and run custom code.&#xA;In particular two attack paths that enabled this with AWS Kiro were identified:</description>
    </item>
    <item>
      <title>How Prompt Injection Exposes Manus&#39; VS Code Server to the Internet</title>
      <link>https://embracethered.com/blog/posts/2025/manus-ai-kill-chain-expose-port-vs-code-server-on-internet/</link>
      <pubDate>Mon, 25 Aug 2025 04:00:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/manus-ai-kill-chain-expose-port-vs-code-server-on-internet/</guid>
      <description>Today we will cover a powerful, easy to use, autonomous agent called Manus. Manus is developed by the Chinese startup Butterfly Effect, headquartered in Singapore.&#xA;This post demonstrates an end-to-end indirect prompt injection attack leading to a compromise of Manus&amp;rsquo; dev box.&#xA;This is achieved by tricking Manus to expose it&amp;rsquo;s internal VS Code Server to the Internet, and then sharing the URL and password with the atacker. Specifically, this post demonstrates that:</description>
    </item>
    <item>
      <title>How Deep Research Agents Can Leak Your Data</title>
      <link>https://embracethered.com/blog/posts/2025/chatgpt-deep-research-connectors-data-spill-and-leaks/</link>
      <pubDate>Sun, 24 Aug 2025 18:03:35 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/chatgpt-deep-research-connectors-data-spill-and-leaks/</guid>
      <description>Recently, many of our favorite AI chatbots have gotten autonomous research capabilities. This allows the AI to go off for an extended period of time, while having access to tools, such as web search, integrations, connectors and also custom-built MCP servers.&#xA;This post will explore and explain in detail how there can be data spill between connected tools during Deep Research. The research is focused on ChatGPT but applies to other Deep Research agents as well.</description>
    </item>
    <item>
      <title>Sneaking Invisible Instructions by Developers in Windsurf</title>
      <link>https://embracethered.com/blog/posts/2025/windsurf-sneaking-invisible-instructions-for-prompt-injection/</link>
      <pubDate>Sat, 23 Aug 2025 16:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/windsurf-sneaking-invisible-instructions-for-prompt-injection/</guid>
      <description>Imagine a malicious instruction hidden in plain sight, invisible to you but not to the AI. This is a vulnerability discovered in Windsurf Cascade, it follows invisible instructions. This means there can be instructions in a file or result of a tool call that the developer cannot see, but the LLM does.&#xA;Some LLMs interpret invisible Unicode Tag characters as instructions, which can lead to hidden prompt injection.&#xA;As far as I can tell the Windsurf SWE-1 model can also &amp;ldquo;see&amp;rdquo; these invisible characters, but the SWE-1 is not yet capable of interpreting them as instructions.</description>
    </item>
    <item>
      <title>Windsurf: Memory-Persistent Data Exfiltration (SpAIware Exploit)</title>
      <link>https://embracethered.com/blog/posts/2025/windsurf-spaiware-exploit-persistent-prompt-injection/</link>
      <pubDate>Fri, 22 Aug 2025 15:21:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/windsurf-spaiware-exploit-persistent-prompt-injection/</guid>
      <description>In this second post about Windsurf Cascade we are exploring the SpAIware attack, which allows memory persistent data exfiltration. SpAIware is an attack we first successfully demonstrated with ChatGPT last year and OpenAI mitigated.&#xA;While inspecting the system prompt of Windsurf Cascade I noticed that it has a create_memory tool.&#xA;Creating Memories The question that immediately popped into my head was if this tool will require human approval when Cascade creates a long-term memory, or if it is added automatically.</description>
    </item>
    <item>
      <title>Hijacking Windsurf: How Prompt Injection Leaks Developer Secrets</title>
      <link>https://embracethered.com/blog/posts/2025/windsurf-data-exfiltration-vulnerabilities/</link>
      <pubDate>Thu, 21 Aug 2025 02:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/windsurf-data-exfiltration-vulnerabilities/</guid>
      <description>This is the first post in a series exploring security vulnerabilities in Windsurf. If you are unfamiliar with Windsurf, it is a fork of VS Code and the coding agent is called Windsurf Cascade.&#xA;The attack vectors we will explore today allow an adversary during an indirect prompt injection to exfiltrate data from the developer&amp;rsquo;s machine.&#xA;These vulnerabilities are a great example of Simon Willison&amp;rsquo;s lethal trifecta pattern.&#xA;Overall, the security vulnerability reporting experience with Windsurf has not been great.</description>
    </item>
    <item>
      <title>Amazon Q Developer for VS Code Vulnerable to Invisible Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/amazon-q-developer-interprets-hidden-instructions/</link>
      <pubDate>Wed, 20 Aug 2025 04:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amazon-q-developer-interprets-hidden-instructions/</guid>
      <description>The Amazon Q Developer VS Code Extension (Amazon Q) is a very popular coding agent, with over 1 million downloads.&#xA;In previous posts we showed how prompt injection vulnerabilities in Amazon Q could lead to:&#xA;Exfiltration of sensitive information from the user&amp;rsquo;s machine , and also to a System compromise by running arbitrary code Today we will show how an attack can leverage invisible Unicode Tag characters that humans cannot see.</description>
    </item>
    <item>
      <title>Amazon Q Developer: Remote Code Execution with Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/amazon-q-developer-remote-code-execution/</link>
      <pubDate>Tue, 19 Aug 2025 14:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amazon-q-developer-remote-code-execution/</guid>
      <description>The Amazon Q Developer VS Code Extension (Amazon Q) is a popular coding agent, with over 1 million downloads.&#xA;The extension is vulnerable to indirect prompt injection, and in this post we discuss a vulnerability that allowed an adversary (or also the AI for that matter) to run arbitrary commands on the host without the developer&amp;rsquo;s consent.&#xA;The resulting impact of the vulnerability is the same as CVE-2025-53773 that Microsoft fixed in GitHub Copilot, however AWS did not issue a CVE when patching the vulnerabiliy.</description>
    </item>
    <item>
      <title>Amazon Q Developer: Secrets Leaked via DNS and Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/amazon-q-developer-data-exfil-via-dns/</link>
      <pubDate>Mon, 18 Aug 2025 12:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amazon-q-developer-data-exfil-via-dns/</guid>
      <description>The next three posts will cover high severity vulnerabilities in the Amazon Q Developer VS Code Extension (Amazon Q Developer), which is a very popular coding agent, with over 1 million downloads.&#xA;It is vulnerable to prompt injection from untrusted data and its security depends heavily on model behavior.&#xA;At a high level Amazon Q Developer can leak sensitive information from a developer&amp;rsquo;s machine, e.g. API keys, to external servers via DNS requests.</description>
    </item>
    <item>
      <title>Data Exfiltration via Image Rendering Fixed in Amp Code</title>
      <link>https://embracethered.com/blog/posts/2025/amp-code-fixed-data-exfiltration-via-images/</link>
      <pubDate>Sun, 17 Aug 2025 04:10:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amp-code-fixed-data-exfiltration-via-images/</guid>
      <description>In this post we discuss a vulnerability that was present in Amp Code from Sourcegraph by which an attacker could exploit markdown driven image rendering to exfiltrate sensitive information.&#xA;This vulnerability is common in AI applications and agents, and it&amp;rsquo;s actually similar to one we discussed last year in GitHub Copilot which Microsoft fixed.&#xA;Exploit Demonstration For the proof-of-concept I use a pre-existing demo that created a longer time ago.</description>
    </item>
    <item>
      <title>Amp Code: Invisible Prompt Injection Fixed by Sourcegraph</title>
      <link>https://embracethered.com/blog/posts/2025/amp-code-fixed-invisible-prompt-injection/</link>
      <pubDate>Sat, 16 Aug 2025 12:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amp-code-fixed-invisible-prompt-injection/</guid>
      <description>In this post we will look at Amp, a coding agent from Sourcegraph. The other day we discussed how invisible instructions impact Google Jules.&#xA;Turns out that many client applications are vulnerable to these kinds of attacks when they use models that support invisible instructions, like Claude.&#xA;Invisible Unicode Tag Characters Interpreted as Instructions We have talked about hidden prompt injections quite a bit in the past, and so I&amp;rsquo;m keeping this short.</description>
    </item>
    <item>
      <title>Google Jules is Vulnerable To Invisible Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/google-jules-invisible-prompt-injection/</link>
      <pubDate>Fri, 15 Aug 2025 02:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/google-jules-invisible-prompt-injection/</guid>
      <description>The latest Gemini models quite reliably interpret hidden Unicode Tag characters as instructions. This vulnerability, first reported to Google over a year ago, has not been mitigated at the model or API level, hence now affects all applications built on top of Gemini.&#xA;This includes Google&amp;rsquo;s own products and services, like Google Jules.&#xA;Hopefully, this post helps raise awareness of this emerging threat.&#xA;Invisible Prompt Injections in GitHub Issues When Jules is asked to work on a task, such as a GitHub issue, it is possible to plant invisible instructions into a GitHub issue to add backdoor code, or have it run arbitrary commands and tools.</description>
    </item>
    <item>
      <title>Jules Zombie Agent: From Prompt Injection to Remote Control</title>
      <link>https://embracethered.com/blog/posts/2025/google-jules-remote-code-execution-zombai/</link>
      <pubDate>Thu, 14 Aug 2025 04:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/google-jules-remote-code-execution-zombai/</guid>
      <description>In the previous post, we explored two data exfiltration vectors that Jules is vulnerable to and that can be exploited via prompt injection. This post takes it further by demonstrating how Jules can be convinced to download malware and join a remote command &amp;amp; control server.&#xA;This research was performed in May 2025 and findings were shared with Google.&#xA;Remote Command &amp;amp; Control - Proof Of Concept The basic attack chain follows the classic AI Kill Chain:</description>
    </item>
    <item>
      <title>Google Jules: Vulnerable to Multiple Data Exfiltration Issues</title>
      <link>https://embracethered.com/blog/posts/2025/google-jules-vulnerable-to-data-exfiltration-issues/</link>
      <pubDate>Wed, 13 Aug 2025 18:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/google-jules-vulnerable-to-data-exfiltration-issues/</guid>
      <description>This post explores data exfiltration attacks in Google Jules, an asynchronous coding agent. This is the first of three posts that will highlight my research on Google Jules in May 2025. All information provided was also shared with Google at that time.&#xA;This first post will focus on data exfiltration, the lethal trifecta.&#xA;But let&amp;rsquo;s first talk about Jules&amp;rsquo; system prompt.&#xA;Jules&amp;rsquo; System Prompt and Multiple Agents To grab the system prompt I just asked it to write it into a file.</description>
    </item>
    <item>
      <title>GitHub Copilot: Remote Code Execution via Prompt Injection (CVE-2025-53773)</title>
      <link>https://embracethered.com/blog/posts/2025/github-copilot-remote-code-execution-via-prompt-injection/</link>
      <pubDate>Tue, 12 Aug 2025 14:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/github-copilot-remote-code-execution-via-prompt-injection/</guid>
      <description>This post is about an important, but also scary, prompt injection discovery that leads to full system compromise of the developer&amp;rsquo;s machine in GitHub Copilot and VS Code.&#xA;It is achieved by placing Copilot into YOLO mode by modifying the project’s settings.json file.&#xA;As described a few days ago with Amp, a vulnerability pattern in agents that might be overlooked is that if an agent can write to files and modify its own configuration or update security-relevant settings it can lead to remote code execution.</description>
    </item>
    <item>
      <title>Claude Code: Data Exfiltration with DNS (CVE-2025-55284)</title>
      <link>https://embracethered.com/blog/posts/2025/claude-code-exfiltration-via-dns-requests/</link>
      <pubDate>Mon, 11 Aug 2025 04:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/claude-code-exfiltration-via-dns-requests/</guid>
      <description>Today we cover Claude Code and a high severity vulnerability that Anthropic fixed in early June. The vulnerability allowed an attacker to hijack Claude Code via indirect prompt injection and leak sensitive information from the developer&amp;rsquo;s machine, e.g. API keys, to external servers by issuing DNS requests.&#xA;Prompt Injection Hijacks Claude When reviewing or interacting with untrusted code or processing data from external systems, Claude Code can be hijacked to run bash commands that allow leaking of sensitive information without user approval.</description>
    </item>
    <item>
      <title>ZombAI Exploit with OpenHands: Prompt Injection To Remote Code Execution</title>
      <link>https://embracethered.com/blog/posts/2025/openhands-remote-code-execution-zombai/</link>
      <pubDate>Sun, 10 Aug 2025 04:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/openhands-remote-code-execution-zombai/</guid>
      <description>Today we have another post about OpenHands from All Hands AI. It is a popular agent, initially named &amp;ldquo;OpenDevin&amp;rdquo;, and recently the company also provides a cloud-based service. Which is all pretty cool and exciting.&#xA;Prompt Injection to Full System Compromise However, as you know, LLM powered apps and agents are vulnerable to prompt injection. That also applies to OpenHands and it can be hijacked by untrusted data, e.g. from a website.</description>
    </item>
    <item>
      <title>OpenHands and the Lethal Trifecta: How Prompt Injection Can Leak Access Tokens</title>
      <link>https://embracethered.com/blog/posts/2025/openhands-the-lethal-trifecta-strikes-again/</link>
      <pubDate>Sat, 09 Aug 2025 03:00:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/openhands-the-lethal-trifecta-strikes-again/</guid>
      <description>Another day, another AI data exfiltration exploit. Today we talk about OpenHands, formerly referred to as OpenDevin. It&amp;rsquo;s created by All-Hands AI.&#xA;The OpenHands agent renders images in chat, which enables zero-click data exfiltration.&#xA;Simon Willison recently gave this data exfiltration attack pattern a great name: Lethal Trifecta.&#xA;We discuss this specific image based attack technique frequently. Sometimes a message must be repeated multiple times to raise awareness and become mainstream knowledge.</description>
    </item>
    <item>
      <title>AI Kill Chain in Action: Devin AI Exposes Ports to the Internet with Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/devin-ai-kill-chain-exposing-ports/</link>
      <pubDate>Fri, 08 Aug 2025 00:02:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/devin-ai-kill-chain-exposing-ports/</guid>
      <description>Today let&amp;rsquo;s explore Devin&amp;rsquo;s system prompt a bit more. Specifically, an interesing tool that I discovered when reading through it.&#xA;Hidden in Devin’s capabilities is a tool that can open any local port to the public Internet. That means, with the right indirect prompt injection nudge, Devin can be tricked into publishing sensitive files or services for anyone to access.&#xA;This tunneling feature can be invoked without a human in the loop.</description>
    </item>
    <item>
      <title>How Devin AI Can Leak Your Secrets via Multiple Means</title>
      <link>https://embracethered.com/blog/posts/2025/devin-can-leak-your-secrets/</link>
      <pubDate>Thu, 07 Aug 2025 08:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/devin-can-leak-your-secrets/</guid>
      <description>In this post we show how an attacker can make Devin send sensitive information to third-party servers, via multiple means. This post assumes that you read the first post about Devin as well.&#xA;But here is a quick recap: During an indirect prompt injection Devin can be tricked into download malware and extract sensitive information on the machine. But there is more&amp;hellip;&#xA;Let&amp;rsquo;s explore how Devin can leak sensitive information and send it to a third-party server.</description>
    </item>
    <item>
      <title>I Spent $500 To Test Devin AI For Prompt Injection So That You Don&#39;t Have To</title>
      <link>https://embracethered.com/blog/posts/2025/devin-i-spent-usd500-to-hack-devin/</link>
      <pubDate>Wed, 06 Aug 2025 01:01:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/devin-i-spent-usd500-to-hack-devin/</guid>
      <description>Today we cover Devin AI from Cognition, the first AI Software Engineer.&#xA;We will cover Devin proof-of-concept exploits in multiple posts over the next few days. In this first post, we show how a prompt injection payload hosted on a website leads to a full compromise of Devin&amp;rsquo;s DevBox.&#xA;GitHub Issue To Remote Code Execution By planting instructions on a website or GitHub issue that Devin processes, it can be tricked to download malware and launch it.</description>
    </item>
    <item>
      <title>Amp Code: Arbitrary Command Execution via Prompt Injection Fixed</title>
      <link>https://embracethered.com/blog/posts/2025/amp-agents-that-modify-system-configuration-and-escape/</link>
      <pubDate>Tue, 05 Aug 2025 06:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/amp-agents-that-modify-system-configuration-and-escape/</guid>
      <description>Sandbox-escape-style attacks can happen when an AI is able to modify its own configuration settings, such as by writing to configuration files.&#xA;That was the case with Amp, an agentic coding tool built by Sourcegraph.&#xA;The AI coding agent could update its own configuration and:&#xA;Allowlist bash commands or Add a malicious MCP server on the fly to run arbitrary code This could have been exploited by the model itself, or during an indirect prompt injection attack as we will demonstrate in this post.</description>
    </item>
    <item>
      <title>Cursor IDE: Arbitrary Data Exfiltration Via Mermaid (CVE-2025-54132)</title>
      <link>https://embracethered.com/blog/posts/2025/cursor-data-exfiltration-with-mermaid/</link>
      <pubDate>Mon, 04 Aug 2025 00:04:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/cursor-data-exfiltration-with-mermaid/</guid>
      <description>Cursor is a popular AI code editor. In this post I want to share how I found an interesting data exfiltration issue, the demo exploits built and how it got fixed.&#xA;When using Cursor I noticed that it can render Mermaid diagrams.&#xA;Cursor Renders Mermaid Diagrams If you are not familiar with Mermaid, it has a simple syntax:&#xA;graph TD User --&amp;gt; Computer This will create a diagram as follows:</description>
    </item>
    <item>
      <title>Anthropic Filesystem MCP Server: Directory Access Bypass via Improper Path Validation</title>
      <link>https://embracethered.com/blog/posts/2025/anthropic-filesystem-mcp-server-bypass/</link>
      <pubDate>Sun, 03 Aug 2025 01:30:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/anthropic-filesystem-mcp-server-bypass/</guid>
      <description>A few months ago I was looking at the filesystem MCP server from Anthropic.&#xA;The server allows to give an AI, like Claude Desktop, access to the local filesystem to read files or edit them and so forth.&#xA;I was curious about access control and in the documentation there is a configuration setting to set allowedDirectories, which the AI should be allowed access to:&#xA;As you can see the example shows two folders being allowlisted for access.</description>
    </item>
    <item>
      <title>Turning ChatGPT Codex Into A ZombAI Agent</title>
      <link>https://embracethered.com/blog/posts/2025/chatgpt-codex-remote-control-zombai/</link>
      <pubDate>Sat, 02 Aug 2025 00:31:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/chatgpt-codex-remote-control-zombai/</guid>
      <description>Today we cover ChatGPT Codex as part of the Month of AI Bugs series.&#xA;ChatGPT Codex is a cloud-based software engineering agent that answers codebase questions, executes code, and drafts pull requests.&#xA;In particular, this post will demonstrate how Codex is vulnerable to prompt injection, and how the use of the &amp;ldquo;Common Dependencies Allowlist&amp;rdquo; for Internet access enables an attacker to recruit ChatGPT Codex into a malware botnet.&#xA;The ZombAI attack arrives at ChatGPT Codex today!</description>
    </item>
    <item>
      <title>Exfiltrating Your ChatGPT Chat History and Memories With Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2025/chatgpt-chat-history-data-exfiltration/</link>
      <pubDate>Fri, 01 Aug 2025 08:00:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/chatgpt-chat-history-data-exfiltration/</guid>
      <description>In this post we demonstrate how a bypass in OpenAI&amp;rsquo;s &amp;ldquo;safe URL&amp;rdquo; rendering feature allows ChatGPT to send personal information to a third-party server. This can be exploited by an adversary via a prompt injection via untrusted data.&#xA;If you process untrusted content, like summarizing a website, or analyze a pdf document, the author of that document can exfiltrate any information present in the prompt context, including your past chat history.</description>
    </item>
    <item>
      <title>The Month of AI Bugs 2025</title>
      <link>https://embracethered.com/blog/posts/2025/announcement-the-month-of-ai-bugs/</link>
      <pubDate>Mon, 28 Jul 2025 10:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/announcement-the-month-of-ai-bugs/</guid>
      <description>This year I spent a lot of time reviewing, exploiting and working with vendors to fix vulnerabilities in agentic AI systems.&#xA;As a result, I&amp;rsquo;m excited to announce the Month of AI Bugs 2025!&#xA;Goal Of The Initiative The main purpose of the Month of AI Bugs is to raise awareness about novel security vulnerabilities in agentic systems, primarily focusing on AI coding agents. Posts will cover both simple and advanced, sometimes even mind-boggling exploits.</description>
    </item>
    <item>
      <title>Security Advisory: Anthropic&#39;s Slack MCP Server Vulnerable to Data Exfiltration</title>
      <link>https://embracethered.com/blog/posts/2025/security-advisory-anthropic-slack-mcp-server-data-leakage/</link>
      <pubDate>Tue, 24 Jun 2025 16:00:46 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/security-advisory-anthropic-slack-mcp-server-data-leakage/</guid>
      <description>This is a security advisory for a data leakage and exfiltration vulnerability in a popular, but now deprecated and unmaintained, Slack MCP Server from Anthropic.&#xA;If you are using this MCP server, or run an &amp;ldquo;MCP Store&amp;rdquo; that hosts it, it is advised that you analyze how this threat applies to your use case and apply a patch as needed.&#xA;Anthropic&amp;rsquo;s Slack MCP Server When Anthropic introduced MCP they published reference server implementations on Github.</description>
    </item>
    <item>
      <title>AI ClickFix: Hijacking Computer-Use Agents Using ClickFix</title>
      <link>https://embracethered.com/blog/posts/2025/ai-clickfix-ttp-claude/</link>
      <pubDate>Sat, 24 May 2025 16:20:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/ai-clickfix-ttp-claude/</guid>
      <description>Today we are going to discuss how real-world tactics, techniques, and procedures (TTPs) apply to computer-use systems, specifically, we&amp;rsquo;ll look at ClickFix attacks. This demo was part of my presentation at the SAGAI Workshop on May 15th, 2025 in San Francisco.&#xA;It was a great workshop, with tons of interesting insights and discussions.&#xA;So, let&amp;rsquo;s talk about ClickFix, and how it applies to AI systems!&#xA;What is ClickFix? ClickFix is a social engineering technique that is being used by adversaries.</description>
    </item>
  </channel>
</rss>
