<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rce on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/rce/</link>
    <description>Recent content in Rce on Embrace The Red</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2026</copyright>
    <lastBuildDate>Wed, 11 Feb 2026 06:00:00 -0700</lastBuildDate>
    <atom:link href="https://embracethered.com/blog/tags/rce/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scary Agent Skills: Hidden Unicode Instructions in Skills ...And How To Catch Them</title>
      <link>https://embracethered.com/blog/posts/2026/scary-agent-skills/</link>
      <pubDate>Wed, 11 Feb 2026 06:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2026/scary-agent-skills/</guid>
      <description>&lt;p&gt;There is a lot of talk about Skills recently, both in terms of capabilities and security concerns. However, so far I haven&amp;rsquo;t seen anyone bring up hidden prompt injection. So, I figured to demo a Skills supply chain backdoor that survives human review.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://embracethered.com/blog/images/2026/skills/scary-agent-skills.png&#34;&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2026/skills/scary-agent-skills.png&#34; alt=&#34;scary agent skills logo&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Additionally, I also built a &lt;a href=&#34;https://github.com/wunderwuzzi23/aid&#34;&gt;basic scanner&lt;/a&gt;, and had &lt;a href=&#34;https://github.com/openclaw/openclaw/pull/13012&#34;&gt;my agent propose updates to OpenClaw&lt;/a&gt; to catch such attacks.&lt;/p&gt;&#xA;&lt;h2 id=&#34;attack-surface&#34;&gt;Attack Surface&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;Skills&lt;/code&gt; introduce common threats, like prompt injection, supply chain attacks, RCE, data exfiltration,&amp;hellip;  This post discusses some basics, highlights the most simple prompt injection avenue, and shows how one can backdoor a real &lt;code&gt;Skill&lt;/code&gt; from OpenAI with invisible &lt;code&gt;Unicode Tag codepoints&lt;/code&gt; that certain models, like Gemini, Claude, Grok are known to interpret as instructions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Antigravity Grounded! Security Vulnerabilities in Google&#39;s Latest IDE</title>
      <link>https://embracethered.com/blog/posts/2025/security-keeps-google-antigravity-grounded/</link>
      <pubDate>Tue, 25 Nov 2025 06:00:58 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/security-keeps-google-antigravity-grounded/</guid>
      <description>&lt;p&gt;Last week Google released an IDE called Antigravity. It&amp;rsquo;s basically the outcome of the Windsurf licensing deal from a few months ago, where &lt;a href=&#34;https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/&#34;&gt;Google paid some $2.4 billion for a non-exclusive license to the code&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Because it&amp;rsquo;s based on Windsurf, I was curious if vulnerabilities that I reported to Windsurf back in May 2025, long before the deal, would have been addressed in the Antigravity IDE. See &lt;a href=&#34;https://embracethered.com/blog/posts/2025/wrapping-up-month-of-ai-bugs/&#34;&gt;Month of AI Bugs&lt;/a&gt; for some detailed write-ups.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
