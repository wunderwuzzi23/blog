<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Huskyai on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/huskyai/</link>
    <description>Recent content in Huskyai on Embrace The Red</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2026</copyright>
    <lastBuildDate>Sat, 18 May 2024 16:00:00 -0700</lastBuildDate>
    <atom:link href="https://embracethered.com/blog/tags/huskyai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning Attack Series: Backdooring Keras Models and How to Detect It</title>
      <link>https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/</link>
      <pubDate>Sat, 18 May 2024 16:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/</guid>
      <description>&lt;p&gt;This post is part of a &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;series&lt;/a&gt; about machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;Adversaries often leverage supply chain attacks to gain footholds. In machine learning &lt;strong&gt;model deserialization issues&lt;/strong&gt; are a significant threat, and detecting them is crucial, as they can lead to arbitrary code execution. We explored this attack with &lt;a href=&#34;https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/&#34;&gt;Python Pickle files in the past&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post we are covering backdooring the original Keras &lt;code&gt;Husky AI&lt;/code&gt; model from the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Machine Learning Attack Series&lt;/a&gt;, and afterwards we investigate tooling to detect the backdoor.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLSecOps Podcast: AI Red Teaming and Threat Modeling Machine Learning Systems</title>
      <link>https://embracethered.com/blog/posts/2023/mlsecops-podcast-ai-red-teaming/</link>
      <pubDate>Thu, 27 Apr 2023 20:59:51 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2023/mlsecops-podcast-ai-red-teaming/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Hack and protect Machine Learning Systems so that we don&amp;rsquo;t get stuck in the matrix!&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;It&amp;rsquo;s been almost three years since I started the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Machine Learning Attack Series&lt;/a&gt;, and my interest in attacking and leveraging AI and Machine Learning is unbroken.&lt;/p&gt;&#xA;&lt;p&gt;There is so much to learn and explore, particularly in bridging the gap between traditional security engineering and machine learning. As followers of this blog will know, this is an area I have been exploring in-depth over the past few years.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Backdooring Pickle Files</title>
      <link>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</link>
      <pubDate>Sun, 28 Aug 2022 20:10:44 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</guid>
      <description>&lt;p&gt;Recently I read &lt;a href=&#34;https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/&#34;&gt;this excellent post by Evan Sultanik&lt;/a&gt; about exploiting pickle files on Trail of Bits. There was also a DefCon30 talk about &lt;a href=&#34;https://forum.defcon.org/node/241825&#34;&gt;backdooring pickle files by ColdwaterQ&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This got me curious to try out backdooring a pickle file myself.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/ml-attack-series.jpg&#34; alt=&#34;Red Teaming Machine Learning -  Attack Series&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;pickle-files---the-surprises&#34;&gt;Pickle files - the surprises&lt;/h1&gt;&#xA;&lt;p&gt;Surprisingly Python pickle files are compiled programs running in a VM called the Pickle Machine (PM). Opcodes control the flow, and when there are opcodes there is often fun to be had.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Video: Understanding Image Scaling Attacks</title>
      <link>https://embracethered.com/blog/posts/2021/video-image-scaling-attacks/</link>
      <pubDate>Tue, 12 Oct 2021 00:02:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2021/video-image-scaling-attacks/</guid>
      <description>&lt;p&gt;Today you are in for a special treat. Did you know that an adversary can hide a smaller image within a larger one?&lt;/p&gt;&#xA;&lt;p&gt;This video demonstrates how a small image becomes magically visible when the computer resizes the large image, and also how to mitigate the vulnerability.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UItbZNBtfaQ&#34;&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2021/image-scaling-attack.png&#34; alt=&#34;Image Scaling Attacks&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This is possible when vulnerable code uses insecure interpolation.&lt;/p&gt;&#xA;&lt;p&gt;If you like this one check out the overall &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Machine Learning Attack Series&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Microsoft Counterfit to create adversarial examples for Husky AI</title>
      <link>https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/</link>
      <pubDate>Mon, 16 Aug 2021 10:00:26 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/</guid>
      <description>&lt;p&gt;This post is part of the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;machine learning attack series&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;It&amp;rsquo;s been a while that I did a Husky AI and offensive machine learning related post. This weekend I had some time to try out &lt;a href=&#34;https://github.com/Azure/counterfit/wiki&#34;&gt;Counterfit&lt;/a&gt;. My goal was to understand what Counterfit is, how it works, and use it to turn Shadowbunny into a husky.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/huskyai-shadowbunny.png&#34; alt=&#34;Shadowbunny&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s get started.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-counterfit&#34;&gt;What is Counterfit?&lt;/h2&gt;&#xA;&lt;p&gt;With Counterfit you can test your machine learning models and endpoints for specific adversarial attacks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Overview </title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</link>
      <pubDate>Thu, 26 Nov 2020 09:00:51 -0800</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</guid>
      <description>&lt;p&gt;What a journey it has been. I wrote quite a bit about machine learning from a red teaming/security testing perspective this year. It was brought to my attention to provide a conveninent &amp;ldquo;index page&amp;rdquo; with all Husky AI and related blog posts. Here it is.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/ml-attack-series.jpg&#34; alt=&#34;ML Attack Series&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;machine-learning-basics-and-building-husky-ai&#34;&gt;Machine Learning Basics and Building Husky AI&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-basics/&#34;&gt;Getting the hang of machine learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/&#34;&gt;The machine learning pipeline and attacks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/&#34;&gt;Husky AI: Building a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/&#34;&gt;MLOps - Operationalizing the machine learning model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;threat-modeling-and-strategies&#34;&gt;Threat Modeling and Strategies&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/&#34;&gt;Threat modeling a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-SV80sIBhqY&#34;&gt;Grayhat Red Team Village Video: Building and breaking a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-assume-bias-strategy/&#34;&gt;Assume Bias and Responsible AI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;practical-attacks-and-defenses&#34;&gt;Practical Attacks and Defenses&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/&#34;&gt;Brute forcing images to find incorrect predictions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/&#34;&gt;Smart brute forcing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/&#34;&gt;Perturbations to misclassify existing images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/&#34;&gt;Adversarial Robustness Toolbox Basics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/&#34;&gt;Image Scaling Attacks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/&#34;&gt;Stealing a model file: Attacker gains read access to the model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/&#34;&gt;Backdooring models: Attacker modifies persisted model file&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/&#34;&gt;Repudiation Threat and Auditing: Catching modifications and unauthorized access&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/&#34;&gt;Attacker modifies Jupyter Notebook file to insert a backdoor&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/&#34;&gt;CVE 2020-16977: VS Code Python Extension Remote Code Execution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/&#34;&gt;Using Generative Adversarial Networks (GANs) to create fake husky images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/&#34;&gt;Using Microsoft Counterfit to create adversarial examples&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/&#34;&gt;Backdooring Pickle Files&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/&#34;&gt;Backdooring Keras Model Files and How to Detect It&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/microsoft-machine-learning-security-evasion-competition/&#34;&gt;Participating in the Microsoft Machine Learning Security Evasion Competition - Bypassing malware models by signing binaries&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/wunderwuzzi23/huskyai/&#34;&gt;Husky AI Github Repo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;As you can see there are many machine learning specific attacks, but also a lot of &amp;ldquo;typical&amp;rdquo; red teaming techniques that put AI/ML systems at risk. For instance well known attacks such as SSH Agent Hijacking, weak access control and widely exposed credentials will likely help achieve objecives during red teaming operations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Generative Adversarial Networks (GANs)</title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/</link>
      <pubDate>Wed, 25 Nov 2020 19:55:15 -0800</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/</guid>
      <description>&lt;p&gt;In this post we will explore Generative Adversarial Networks (GANs) to create fake husky images. The goal is, of course, to have &amp;ldquo;Husky AI&amp;rdquo; misclassify them as real huskies.&lt;/p&gt;&#xA;&lt;p&gt;If you want to learn more about Husky AI visit the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/&#34;&gt;Overview&lt;/a&gt; post.&lt;/p&gt;&#xA;&lt;h2 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/h2&gt;&#xA;&lt;p&gt;One of the attacks I wanted to investigate for a while was the creation of fake images to trick Husky AI. The best approach seemed by using Generative Adversarial Networks (GANs). &lt;a href=&#34;https://www.deeplearning.ai/generative-adversarial-networks-specialization/&#34;&gt;It happened that right then deeplearning.ai started offering a GAN course by Sharon Zhou&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Repudiation Threat and Auditing</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/</link>
      <pubDate>Tue, 10 Nov 2020 16:00:21 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Attacks&lt;/a&gt;: The attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In this post we are going to look at the &amp;ldquo;Repudiation Threat&amp;rdquo;, which is one of the threats often overlooked when performing threat modeling, and maybe something you would not even expect in a series about machine learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Video: Building and breaking a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/learning-by-doing-building-and-breaking-machine-learning-red-team-hacking/</link>
      <pubDate>Thu, 05 Nov 2020 15:30:00 -0800</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/learning-by-doing-building-and-breaking-machine-learning-red-team-hacking/</guid>
      <description>&lt;p&gt;My GrayHat Red Team Village talk &amp;ldquo;Learning by doing: Building and breaking a machine learning system&amp;rdquo; is now live on YouTube.&lt;/p&gt;&#xA;&lt;p&gt;Check it out: &lt;a href=&#34;https://www.youtube.com/watch?v=-SV80sIBhqY&#34;&gt;https://www.youtube.com/watch?v=-SV80sIBhqY&lt;/a&gt; and smash the Like button! :D&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-SV80sIBhqY&#34;&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/grayhat-video.png&#34; alt=&#34;Red Team Village Talk&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question?&lt;/h3&gt;&#xA;&lt;p&gt;I thought of turning the content into a hands-on workshop. Let me know if that would be something that would you would attend? Trying to see if there is interest.&lt;/p&gt;&#xA;&lt;p&gt;Cheers,&#xA;Johann&lt;/p&gt;&#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/wunderwuzzi23&#34;&gt;@wunderwuzzi23&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/rtv-2020-icon-servericon.png&#34; alt=&#34;Red Team Village&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Image Scaling Attacks</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</link>
      <pubDate>Wed, 28 Oct 2020 13:00:27 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/&#34;&gt;Attacks&lt;/a&gt;: Some of the attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;A few weeks ago while preparing demos for my GrayHat 2020 - Red Team Village presentation I ran across &amp;ldquo;Image Scaling Attacks&amp;rdquo; in &lt;a href=&#34;https://www.usenix.org/system/files/sec20-quiring.pdf&#34;&gt;Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning&lt;/a&gt; by Erwin Quiring, et al.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Adversarial Robustness Toolbox Basics</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/</link>
      <pubDate>Thu, 22 Oct 2020 15:00:48 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/&#34;&gt;Attacks&lt;/a&gt;: Some of the attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I wanted to explore the &amp;ldquo;Adversarial Robustness Toolbox&amp;rdquo; (ART) for a while to understand how it can be used to create adversarial examples for Husky AI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CVE 2020-16977: VS Code Python Extension Remote Code Execution</title>
      <link>https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/</link>
      <pubDate>Wed, 14 Oct 2020 10:35:02 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/</guid>
      <description>&lt;p&gt;While building &amp;ldquo;Husky AI&amp;rdquo; I started working a lot with Microsoft&amp;rsquo;s VS Code Python extension. It is a super convinient way to edit Jupyter Notebooks. I just use VS Code&amp;rsquo;s Remote SSH feature to get to my Linux host and work on modeling and testing there.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/vscode-notebook.png&#34; alt=&#34;VS Code Python Extension&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;When &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning&#34;&gt;threat modeling &amp;ldquo;Husky AI&amp;rdquo;&lt;/a&gt; I identified backdooring of third party libraries and development tools as a potential issue to be aware of.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Stealing a model file</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/</link>
      <pubDate>Sat, 10 Oct 2020 05:50:21 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Attacks&lt;/a&gt;: The attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We talked about creating adversarial examples and &amp;ldquo;backdoor images&amp;rdquo; for Husky AI before. One thing that we noticed was that an adversary with model access can very efficiently come up with adversarial examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Backdooring models</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/</link>
      <pubDate>Fri, 18 Sep 2020 14:59:47 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Attacks&lt;/a&gt;: The attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mitigations&#34;&gt;Mitigations&lt;/a&gt;: Ways to prevent and detect the backdooring threat&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;During threat modeling we identified that an adversary might tamper with model files. From a technical point of view this means an adversary gained access to the model file used in production and is able overwrite it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Perturbations to misclassify existing images</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/</link>
      <pubDate>Wed, 16 Sep 2020 12:00:05 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Attacks&lt;/a&gt;: The attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The previous post covered some neat smart fuzzing techniques to improve generation of fake husky images.&lt;/p&gt;&#xA;&lt;p&gt;The goal of this post is to take an existing image of the plush bunny below, modify it and have the model identify it as a husky.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Smart brute forcing</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/</link>
      <pubDate>Sun, 13 Sep 2020 09:04:09 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts. There are the two main sections of the series - more content will be added over time:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Overview&lt;/a&gt;: How Husky AI was built, threat modeled and operationalized&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Attacks&lt;/a&gt;: The attacks I want to investigate, learn about, and try out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The previous post covered basic tests to trick the image recognition model. This included generating &amp;ldquo;bad&amp;rdquo; images with solid colors or entire random pixels.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Brute forcing images to find incorrect predictions</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/</link>
      <pubDate>Wed, 09 Sep 2020 09:09:09 -0909</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/&#34;&gt;previous four posts&lt;/a&gt; explained the architecture and how Husky AI was built, threat modeled and deployed. Now itâ€™s time to start the attacks and build mitigations. The &lt;a href=&#34;#appendix&#34;&gt;appendix&lt;/a&gt; in this post shows all the attacks I want to research and perform in this series over the next few weeks/months.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Threat modeling a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &lt;a href=&#34;https://embracethered.com/blog/tags/huskyai/&#34;&gt;&amp;ldquo;huskyai&amp;rdquo;&lt;/a&gt; to see all the posts, or visit the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;machine learning attack series overview section&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/&#34;&gt;previous post&lt;/a&gt; we walked through the steps required to gather training data, build and test a model to build &amp;ldquo;Husky AI&amp;rdquo;.&lt;/p&gt;&#xA;&lt;p&gt;This post is all about threat modeling the system to identify scenarios for attacks which we will perform in the upcoming posts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLOps - Operationalizing the machine learning model</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/</link>
      <pubDate>Sat, 05 Sep 2020 08:00:14 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/</guid>
      <description>&lt;p&gt;This post is part of a &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;series&lt;/a&gt; about machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/&#34;&gt;previous post&lt;/a&gt; we walked through the steps required to gather training data, build and test a model.&lt;/p&gt;&#xA;&lt;p&gt;In this post we dive into &amp;ldquo;Operationalizing&amp;rdquo; the model. The scenario is the creation of Husky AI and my experiences and learnings from that.&lt;/p&gt;&#xA;&lt;h1 id=&#34;part3&#34;&gt;Part 3 - Operationalizing the Husky AI model&lt;/h1&gt;&#xA;&lt;p&gt;This actually took much longer than planned.&lt;/p&gt;&#xA;&lt;p&gt;Since I used TensorFlow, I naively thought it would be very straight forward to implement a Golang web server to host the model. Turns out that TensorFlow/Keras is not that as straightforward to integrate with Golang, it requires a lot of extra steps. So, I ended up picking Python for the web server.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Husky AI: Building a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/</link>
      <pubDate>Fri, 04 Sep 2020 12:04:29 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/</guid>
      <description>&lt;p&gt;This post is part of a series about machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/&#34;&gt;previous post&lt;/a&gt; we described the overall machine learning pipeline.&lt;/p&gt;&#xA;&lt;p&gt;In this post we dive into the technical details on how I built and trained the machine learning model for Husky AI.&lt;/p&gt;&#xA;&lt;p&gt;After reading this you should have a good understanding around the technical steps involved in building a machine learning system, and also some thoughts around what can be attacked.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The machine learning pipeline and attacks</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/</link>
      <pubDate>Wed, 02 Sep 2020 12:04:29 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/</guid>
      <description>&lt;p&gt;This post is part of a &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;series&lt;/a&gt; about machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-basics/&#34;&gt;previous post&lt;/a&gt; I talked about good resources for learning more about artificial intelligence and machine learning in general, and how I started my journey in this space.&lt;/p&gt;&#xA;&lt;p&gt;The next few posts will be about Husky AI.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-husky-ai&#34;&gt;What is Husky AI?&lt;/h2&gt;&#xA;&lt;p&gt;Husky AI allows a user to upload an image, and get an answer back if the image contains a husky or not. Below is a screenshot of the application:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting the hang of machine learning</title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-basics/</link>
      <pubDate>Tue, 01 Sep 2020 18:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-basics/</guid>
      <description>&lt;p&gt;This year I have spent a lot of time studying machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;To come up with good and useful attacks during operations, I figured it is time to learn the fundamentals and start using software, tools and algorithms. My goal was to build a couple of end to end machine learning systems from scratch, and then attack them.&lt;/p&gt;&#xA;&lt;p&gt;This post describes my studying approach, materials, courses, and learnings. I thought to share this, in case there are others who are interested to get started in this space but don&amp;rsquo;t how and where.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
