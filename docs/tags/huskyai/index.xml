<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>huskyai on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/huskyai/</link>
    <description>Recent content in huskyai on Embrace The Red</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2023</copyright>
    <lastBuildDate>Thu, 27 Apr 2023 20:59:51 -0700</lastBuildDate><atom:link href="https://embracethered.com/blog/tags/huskyai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MLSecOps Podcast: AI Red Teaming and Threat Modeling Machine Learning Systems</title>
      <link>https://embracethered.com/blog/posts/2023/mlsecops-podcast-ai-red-teaming/</link>
      <pubDate>Thu, 27 Apr 2023 20:59:51 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2023/mlsecops-podcast-ai-red-teaming/</guid>
      <description>Hack and protect Machine Learning Systems so that we don&amp;rsquo;t get stuck in the matrix!
It&amp;rsquo;s been almost three years since I started the Machine Learning Attack Series, and my interest in attacking and leveraging AI and Machine Learning is unbroken.
There is so much to learn and explore, particularly in bridging the gap between traditional security engineering and machine learning. As followers of this blog will know, this is an area I have been exploring in-depth over the past few years.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Backdooring Pickle Files</title>
      <link>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</link>
      <pubDate>Sun, 28 Aug 2022 20:10:44 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</guid>
      <description>Recently I read this excellent post by Evan Sultanik about exploiting pickle files on Trail of Bits. There was also a DefCon30 talk about backdooring pickle files by ColdwaterQ.
This got me curious to try out backdooring a pickle file myself.
Pickle files - the surprises Surprisingly Python pickle files are compiled programs running in a VM called the Pickle Machine (PM). Opcodes control the flow, and when there are opcodes there is often fun to be had.</description>
    </item>
    
    <item>
      <title>Video: Understanding Image Scaling Attacks</title>
      <link>https://embracethered.com/blog/posts/2021/video-image-scaling-attacks/</link>
      <pubDate>Tue, 12 Oct 2021 00:02:00 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2021/video-image-scaling-attacks/</guid>
      <description>Today you are in for a special treat. Did you know that an adversary can hide a smaller image within a larger one?
This video demonstrates how a small image becomes magically visible when the computer resizes the large image, and also how to mitigate the vulnerability.

This is possible when vulnerable code uses insecure interpolation.
If you like this one check out the overall Machine Learning Attack Series.</description>
    </item>
    
    <item>
      <title>Using Microsoft Counterfit to create adversarial examples for Husky AI</title>
      <link>https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/</link>
      <pubDate>Mon, 16 Aug 2021 10:00:26 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/</guid>
      <description>This post is part of the machine learning attack series.
It&amp;rsquo;s been a while that I did a Husky AI and offensive machine learning related post. This weekend I had some time to try out Counterfit. My goal was to understand what Counterfit is, how it works, and use it to turn Shadowbunny into a husky.
Let&amp;rsquo;s get started.
What is Counterfit? With Counterfit you can test your machine learning models and endpoints for specific adversarial attacks.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Overview </title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</link>
      <pubDate>Thu, 26 Nov 2020 09:00:51 -0800</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</guid>
      <description>What a journey it has been. I wrote quite a bit about machine learning from a red teaming/security testing perspective this year. It was brought to my attention to provide a conveninent &amp;ldquo;index page&amp;rdquo; with all Husky AI and related blog posts. Here it is.
Machine Learning Basics and Building Husky AI  Getting the hang of machine learning The machine learning pipeline and attacks Husky AI: Building a machine learning system MLOps - Operationalizing the machine learning model  Threat Modeling and Strategies  Threat modeling a machine learning system Grayhat Red Team Village Video: Building and breaking a machine learning system Assume Bias and Responsible AI  Practical Attacks and Defenses  Brute forcing images to find incorrect predictions Smart brute forcing Perturbations to misclassify existing images Adversarial Robustness Toolbox Basics Image Scaling Attacks Stealing a model file: Attacker gains read access to the model Backdooring models: Attacker modifies persisted model file Repudiation Threat and Auditing: Catching modifications and unauthorized access Attacker modifies Jupyter Notebook file to insert a backdoor CVE 2020-16977: VS Code Python Extension Remote Code Execution Using Generative Adversarial Networks (GANs) to create fake husky images Using Microsoft Counterfit to create adversarial examples Backdooring Pickle Files  Miscellaneous  Participating in the Microsoft Machine Learning Security Evasion Competition - Bypassing malware models by signing binaries Husky AI Github Repo  Conclusion As you can see there are many machine learning specific attacks, but also a lot of &amp;ldquo;typical&amp;rdquo; red teaming techniques that put AI/ML systems at risk.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Generative Adversarial Networks (GANs)</title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/</link>
      <pubDate>Wed, 25 Nov 2020 19:55:15 -0800</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/</guid>
      <description>In this post we will explore Generative Adversarial Networks (GANs) to create fake husky images. The goal is, of course, to have &amp;ldquo;Husky AI&amp;rdquo; misclassify them as real huskies.
If you want to learn more about Husky AI visit the Overview post.
Generative Adversarial Networks One of the attacks I wanted to investigate for a while was the creation of fake images to trick Husky AI. The best approach seemed by using Generative Adversarial Networks (GANs).</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Repudiation Threat and Auditing</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/</link>
      <pubDate>Tue, 10 Nov 2020 16:00:21 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: The attacks I want to investigate, learn about, and try out  In this post we are going to look at the &amp;ldquo;Repudiation Threat&amp;rdquo;, which is one of the threats often overlooked when performing threat modeling, and maybe something you would not even expect in a series about machine learning.</description>
    </item>
    
    <item>
      <title>Video: Building and breaking a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/learning-by-doing-building-and-breaking-machine-learning-red-team-hacking/</link>
      <pubDate>Thu, 05 Nov 2020 15:30:00 -0800</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/learning-by-doing-building-and-breaking-machine-learning-red-team-hacking/</guid>
      <description>My GrayHat Red Team Village talk &amp;ldquo;Learning by doing: Building and breaking a machine learning system&amp;rdquo; is now live on YouTube.
Check it out: https://www.youtube.com/watch?v=-SV80sIBhqY and smash the Like button! :D

Question? I thought of turning the content into a hands-on workshop. Let me know if that would be something that would you would attend? Trying to see if there is interest.
Cheers, Johann
Twitter: @wunderwuzzi23</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Image Scaling Attacks</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</link>
      <pubDate>Wed, 28 Oct 2020 13:00:27 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: Some of the attacks I want to investigate, learn about, and try out  A few weeks ago while preparing demos for my GrayHat 2020 - Red Team Village presentation I ran across &amp;ldquo;Image Scaling Attacks&amp;rdquo; in Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning by Erwin Quiring, et al.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Adversarial Robustness Toolbox Basics</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/</link>
      <pubDate>Thu, 22 Oct 2020 15:00:48 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: Some of the attacks I want to investigate, learn about, and try out  I wanted to explore the &amp;ldquo;Adversarial Robustness Toolbox&amp;rdquo; (ART) for a while to understand how it can be used to create adversarial examples for Husky AI.</description>
    </item>
    
    <item>
      <title>CVE 2020-16977: VS Code Python Extension Remote Code Execution</title>
      <link>https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/</link>
      <pubDate>Wed, 14 Oct 2020 10:35:02 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/</guid>
      <description>While building &amp;ldquo;Husky AI&amp;rdquo; I started working a lot with Microsoft&amp;rsquo;s VS Code Python extension. It is a super convinient way to edit Jupyter Notebooks. I just use VS Code&amp;rsquo;s Remote SSH feature to get to my Linux host and work on modeling and testing there.
When threat modeling &amp;ldquo;Husky AI&amp;rdquo; I identified backdooring of third party libraries and development tools as a potential issue to be aware of.
So finding security issues in the tools is naturally something I am keeping an eye on.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Stealing a model file</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/</link>
      <pubDate>Sat, 10 Oct 2020 05:50:21 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: The attacks I want to investigate, learn about, and try out  We talked about creating adversarial examples and &amp;ldquo;backdoor images&amp;rdquo; for Husky AI before. One thing that we noticed was that an adversary with model access can very efficiently come up with adversarial examples.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Backdooring models</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/</link>
      <pubDate>Fri, 18 Sep 2020 14:59:47 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: The attacks I want to investigate, learn about, and try out Mitigations: Ways to prevent and detect the backdooring threat  During threat modeling we identified that an adversary might tamper with model files. From a technical point of view this means an adversary gained access to the model file used in production and is able overwrite it.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Perturbations to misclassify existing images</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/</link>
      <pubDate>Wed, 16 Sep 2020 12:00:05 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: The attacks I want to investigate, learn about, and try out  The previous post covered some neat smart fuzzing techniques to improve generation of fake husky images.
The goal of this post is to take an existing image of the plush bunny below, modify it and have the model identify it as a husky.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Smart brute forcing</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/</link>
      <pubDate>Sun, 13 Sep 2020 09:04:09 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts. There are the two main sections of the series - more content will be added over time:
 Overview: How Husky AI was built, threat modeled and operationalized Attacks: The attacks I want to investigate, learn about, and try out  The previous post covered basic tests to trick the image recognition model.</description>
    </item>
    
    <item>
      <title>Machine Learning Attack Series: Brute forcing images to find incorrect predictions</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/</link>
      <pubDate>Wed, 09 Sep 2020 09:09:09 -0909</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see related posts.
The previous four posts explained the architecture and how Husky AI was built, threat modeled and deployed. Now itâ€™s time to start the attacks and build mitigations. The appendix in this post shows all the attacks I want to research and perform in this series over the next few weeks/months.</description>
    </item>
    
    <item>
      <title>Threat modeling a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &amp;ldquo;huskyai&amp;rdquo; to see all the posts, or visit the machine learning attack series overview section.
In the previous post we walked through the steps required to gather training data, build and test a model to build &amp;ldquo;Husky AI&amp;rdquo;.
This post is all about threat modeling the system to identify scenarios for attacks which we will perform in the upcoming posts.</description>
    </item>
    
    <item>
      <title>MLOps - Operationalizing the machine learning model</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/</link>
      <pubDate>Sat, 05 Sep 2020 08:00:14 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence.
In the previous post we walked through the steps required to gather training data, build and test a model.
In this post we dive into &amp;ldquo;Operationalizing&amp;rdquo; the model. The scenario is the creation of Husky AI and my experiences and learnings from that.
Part 3 - Operationalizing the Husky AI model This actually took much longer than planned.</description>
    </item>
    
    <item>
      <title>Husky AI: Building a machine learning system</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/</link>
      <pubDate>Fri, 04 Sep 2020 12:04:29 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence.
In the previous post we described the overall machine learning pipeline.
In this post we dive into the technical details on how I built and trained the machine learning model for Husky AI.
After reading this you should have a good understanding around the technical steps involved in building a machine learning system, and also some thoughts around what can be attacked.</description>
    </item>
    
    <item>
      <title>The machine learning pipeline and attacks</title>
      <link>https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/</link>
      <pubDate>Wed, 02 Sep 2020 12:04:29 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/</guid>
      <description>This post is part of a series about machine learning and artificial intelligence.
In the previous post I talked about good resources for learning more about artificial intelligence and machine learning in general, and how I started my journey in this space.
The next few posts will be about Husky AI.
What is Husky AI? Husky AI allows a user to upload an image, and get an answer back if the image contains a husky or not.</description>
    </item>
    
    <item>
      <title>Getting the hang of machine learning</title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-basics/</link>
      <pubDate>Tue, 01 Sep 2020 18:00:00 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-basics/</guid>
      <description>This year I have spent a lot of time studying machine learning and artificial intelligence.
To come up with good and useful attacks during operations, I figured it is time to learn the fundamentals and start using software, tools and algorithms. My goal was to build a couple of end to end machine learning systems from scratch, and then attack them.
This post describes my studying approach, materials, courses, and learnings.</description>
    </item>
    
  </channel>
</rss>
