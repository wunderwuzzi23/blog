<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/ai/</link>
    <description>Recent content in Ai on Embrace The Red</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2026</copyright>
    <lastBuildDate>Mon, 10 Feb 2025 06:30:21 -0800</lastBuildDate>
    <atom:link href="https://embracethered.com/blog/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hacking Gemini&#39;s Memory with Prompt Injection and Delayed Tool Invocation</title>
      <link>https://embracethered.com/blog/posts/2025/gemini-memory-persistence-prompt-injection/</link>
      <pubDate>Mon, 10 Feb 2025 06:30:21 -0800</pubDate>
      <guid>https://embracethered.com/blog/posts/2025/gemini-memory-persistence-prompt-injection/</guid>
      <description>&lt;p&gt;&lt;em&gt;Imagine your AI rewriting your personal history&amp;hellip;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;A while ago Google added memories to Gemini. Memories allow Gemini to store user-related data across sessions, storing information in long-term memory. The feature is only available to users &lt;a href=&#34;https://support.google.com/gemini/answer/15637730?visit_id=638747979741490779-2881515340&amp;amp;p=saved_info&amp;amp;rd=1&#34;&gt;who subscribe to Gemini Advanced&lt;/a&gt; so far. So, in the fall of last year I chimed in and paid for the subscription for a month to check it out.&lt;/p&gt;&#xA;&lt;p&gt;As a user you can see what Gemini stored about you at &lt;code&gt;https://gemini.google.com/saved-info&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks</title>
      <link>https://embracethered.com/blog/posts/2024/chatgpt-persistent-denial-of-service/</link>
      <pubDate>Mon, 08 Jul 2024 14:30:18 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2024/chatgpt-persistent-denial-of-service/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Imagine you visit a website with ChatGPT, and suddenly, it stops working entirely!&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this post we show how an attacker can use prompt injection to cause a persistent denial of service that lasts across chat sessions for a user.&lt;/p&gt;&#xA;&lt;h2 id=&#34;hacking-memories&#34;&gt;Hacking Memories&lt;/h2&gt;&#xA;&lt;p&gt;Previously we discussed how &lt;a href=&#34;https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/&#34;&gt;ChatGPT is vulnerable to automatic tool invocation of the memory tool&lt;/a&gt;. This can be used by an attacker during prompt injection to ingest malicious or fake memories into your ChatGPT.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations</title>
      <link>https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/</link>
      <pubDate>Tue, 28 May 2024 20:57:38 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2024/llm-apps-automatic-tool-invocations/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/&#34;&gt;previous post&lt;/a&gt; we demonstrated how instructions embedded in untrusted data can invoke ChatGPT&amp;rsquo;s memory tool. The examples we looked at included &lt;code&gt;Uploaded Files&lt;/code&gt;, &lt;code&gt;Connected Apps&lt;/code&gt; and also the &lt;code&gt;Browsing&lt;/code&gt; tool.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://embracethered.com/blog/images/2024//chatgpt-ati2.png&#34;&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2024/chatgpt-ati2.png&#34; alt=&#34;image tool invocation&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;When it came to the browsing tool we observed that mitigations were put in place and older demo exploits did not work anymore. After chatting with other security researchers, I learned that they had observed the same.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ChatGPT: Hacking Memories with Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/</link>
      <pubDate>Wed, 22 May 2024 12:24:07 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://openai.com/index/memory-and-new-controls-for-chatgpt/&#34;&gt;OpenAI recently introduced a memory feature in ChatGPT&lt;/a&gt;, enabling it to recall information across sessions, creating a more personalized user experience.&lt;/p&gt;&#xA;&lt;p&gt;However, with this new capability comes risks. Imagine if an attacker could manipulate your AI assistant (chatbot or agent) to remember false information, bias or even instructions, or delete all your memories! This is not a futuristic scenario, the attack that makes this possible is called &lt;a href=&#34;https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/&#34;&gt;Indirect Prompt Injection&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2024/chatgpt-mem-thumbnail-pi.png&#34; alt=&#34;chatgpt memory logo&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Backdooring Keras Models and How to Detect It</title>
      <link>https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/</link>
      <pubDate>Sat, 18 May 2024 16:00:00 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/</guid>
      <description>&lt;p&gt;This post is part of a &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;series&lt;/a&gt; about machine learning and artificial intelligence.&lt;/p&gt;&#xA;&lt;p&gt;Adversaries often leverage supply chain attacks to gain footholds. In machine learning &lt;strong&gt;model deserialization issues&lt;/strong&gt; are a significant threat, and detecting them is crucial, as they can lead to arbitrary code execution. We explored this attack with &lt;a href=&#34;https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/&#34;&gt;Python Pickle files in the past&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post we are covering backdooring the original Keras &lt;code&gt;Husky AI&lt;/code&gt; model from the &lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/&#34;&gt;Machine Learning Attack Series&lt;/a&gt;, and afterwards we investigate tooling to detect the backdoor.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Backdooring Pickle Files</title>
      <link>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</link>
      <pubDate>Sun, 28 Aug 2022 20:10:44 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/</guid>
      <description>&lt;p&gt;Recently I read &lt;a href=&#34;https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/&#34;&gt;this excellent post by Evan Sultanik&lt;/a&gt; about exploiting pickle files on Trail of Bits. There was also a DefCon30 talk about &lt;a href=&#34;https://forum.defcon.org/node/241825&#34;&gt;backdooring pickle files by ColdwaterQ&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This got me curious to try out backdooring a pickle file myself.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/ml-attack-series.jpg&#34; alt=&#34;Red Teaming Machine Learning -  Attack Series&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;pickle-files---the-surprises&#34;&gt;Pickle files - the surprises&lt;/h1&gt;&#xA;&lt;p&gt;Surprisingly Python pickle files are compiled programs running in a VM called the Pickle Machine (PM). Opcodes control the flow, and when there are opcodes there is often fun to be had.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPT-3 and Phishing Attacks</title>
      <link>https://embracethered.com/blog/posts/2022/gpt-3-ai-and-phishing-attacks/</link>
      <pubDate>Mon, 11 Apr 2022 08:00:43 -0700</pubDate>
      <guid>https://embracethered.com/blog/posts/2022/gpt-3-ai-and-phishing-attacks/</guid>
      <description>&lt;p&gt;In this post, we&amp;rsquo;ll examine how GPT-3 could be used by red teams or adversaries to perform successful phishing attacks. We&amp;rsquo;ll also discuss some potential countermeasures that organizations can take to protect themselves against this type of threat.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-gpt-3&#34;&gt;What is GPT-3?&lt;/h2&gt;&#xA;&lt;p&gt;GPT-3 is a neural network-based machine learning system that was developed by OpenAI, a research lab focused on artificial intelligence. It is designed to generate text that sounds realistic and human-like, and it has been trained on a large corpus of text, including billions of words from the internet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Attack Series: Overview </title>
      <link>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</link>
      <pubDate>Thu, 26 Nov 2020 09:00:51 -0800</pubDate>
      <guid>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</guid>
      <description>&lt;p&gt;What a journey it has been. I wrote quite a bit about machine learning from a red teaming/security testing perspective this year. It was brought to my attention to provide a conveninent &amp;ldquo;index page&amp;rdquo; with all Husky AI and related blog posts. Here it is.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://embracethered.com/blog/images/2020/ml-attack-series.jpg&#34; alt=&#34;ML Attack Series&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;machine-learning-basics-and-building-husky-ai&#34;&gt;Machine Learning Basics and Building Husky AI&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-basics/&#34;&gt;Getting the hang of machine learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/&#34;&gt;The machine learning pipeline and attacks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-building-the-machine-learning-model/&#34;&gt;Husky AI: Building a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-mlops-operationalize-the-model/&#34;&gt;MLOps - Operationalizing the machine learning model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;threat-modeling-and-strategies&#34;&gt;Threat Modeling and Strategies&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/&#34;&gt;Threat modeling a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-SV80sIBhqY&#34;&gt;Grayhat Red Team Village Video: Building and breaking a machine learning system&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-assume-bias-strategy/&#34;&gt;Assume Bias and Responsible AI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;practical-attacks-and-defenses&#34;&gt;Practical Attacks and Defenses&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/&#34;&gt;Brute forcing images to find incorrect predictions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/&#34;&gt;Smart brute forcing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/&#34;&gt;Perturbations to misclassify existing images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-adversarial-robustness-toolbox-testing/&#34;&gt;Adversarial Robustness Toolbox Basics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/&#34;&gt;Image Scaling Attacks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-model-stealing/&#34;&gt;Stealing a model file: Attacker gains read access to the model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/&#34;&gt;Backdooring models: Attacker modifies persisted model file&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/husky-ai-repudiation-threat-deny-action-machine-learning/&#34;&gt;Repudiation Threat and Auditing: Catching modifications and unauthorized access&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/&#34;&gt;Attacker modifies Jupyter Notebook file to insert a backdoor&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/cve-2020-16977-vscode-microsoft-python-extension-remote-code-execution/&#34;&gt;CVE 2020-16977: VS Code Python Extension Remote Code Execution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/machine-learning-attack-series-generative-adversarial-networks-gan/&#34;&gt;Using Generative Adversarial Networks (GANs) to create fake husky images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2021/huskyai-using-azure-counterfit/&#34;&gt;Using Microsoft Counterfit to create adversarial examples&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2022/machine-learning-attack-series-injecting-code-pickle-files/&#34;&gt;Backdooring Pickle Files&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/&#34;&gt;Backdooring Keras Model Files and How to Detect It&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://embracethered.com/blog/posts/2020/microsoft-machine-learning-security-evasion-competition/&#34;&gt;Participating in the Microsoft Machine Learning Security Evasion Competition - Bypassing malware models by signing binaries&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/wunderwuzzi23/huskyai/&#34;&gt;Husky AI Github Repo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;As you can see there are many machine learning specific attacks, but also a lot of &amp;ldquo;typical&amp;rdquo; red teaming techniques that put AI/ML systems at risk. For instance well known attacks such as SSH Agent Hijacking, weak access control and widely exposed credentials will likely help achieve objecives during red teaming operations.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
